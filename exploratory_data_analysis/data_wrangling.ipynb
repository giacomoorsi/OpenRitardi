{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling \n",
    "We explore the data to get a better understanding. The data is in `json` format. We want to covert the data to a table, in which every row is a stop of a train (e.g. the train RE 123 from Milano to Rome stopped at the station Firenze with a delay of 5 minutes). This should make queries easier and more efficient. The size of the data will grow but we will store in columnar format to enable compression. We rename all the columns in order to have a meaningful yet simple naming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "First, you need to download the data. In order to do so, go to [this](https://mega.nz/#F!vIAyDaTJ!PcLTFDbKaJaa0FZIEh5E-w) folder and download all (or only some) of the JSON files. Then, put them in the `DATA_FOLDER`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/Users/giacomoorsi/MEGAsync Downloads/Trenitalia-GenMar2023\"\n",
    "file = \"dati_02_09_2022.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import lit, col, to_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Trenitalia\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# set driver memory to 4GB\n",
    "spark.sparkContext._conf.setAll([('spark.driver.memory', '4g')])\n",
    "\n",
    "# get sc \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema is as follows:\n",
    "```\n",
    "{'type': 'struct',\n",
    " 'fields': [{'name': 'avvisiRFI',\n",
    "   'type': {'type': 'array',\n",
    "    'elementType': {'type': 'struct',\n",
    "     'fields': [{'name': 'corpo',\n",
    "       'type': 'string',\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'data', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'link', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'titolo', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "    'containsNull': True},\n",
    "   'nullable': True,\n",
    "   'metadata': {}},\n",
    "  {'name': 'avvisiTI',\n",
    "   'type': {'type': 'array',\n",
    "    'elementType': {'type': 'struct',\n",
    "     'fields': [{'name': 'corpo',\n",
    "       'type': 'string',\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'data', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'titolo', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "    'containsNull': True},\n",
    "   'nullable': True,\n",
    "   'metadata': {}},\n",
    "  {'name': 'giorno', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "  {'name': 'riassunto',\n",
    "   'type': {'type': 'struct',\n",
    "    'fields': [{'name': 'anticipoAccumulato',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'arrivoAnticipo',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'arrivoInOrario',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'arrivoInRitardo',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'circolazione',\n",
    "      'type': 'string',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'dataAggiornamento',\n",
    "      'type': 'string',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'numEC', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numEN', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numES', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numIC', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numNC', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numREG', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'partenzaInOrario',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'partenzaRitardo',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'ritardoAccumulato',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'ritardoArrivoTrenoPeggiore',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'ritardoPartenzaTrenoPeggiore',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniCancellati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniCircolati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniMonitorati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniRegolari',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniRiprogrammati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'trenoPeggiore',\n",
    "      'type': 'string',\n",
    "      'nullable': True,\n",
    "      'metadata': {}}]},\n",
    "   'nullable': True,\n",
    "   'metadata': {}},\n",
    "  {'name': 'timeZone', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "  {'name': 'treni',\n",
    "   'type': {'type': 'array',\n",
    "    'elementType': {'type': 'struct',\n",
    "     'fields': [{'name': 'a',\n",
    "       'type': 'string',\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'c', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'cn', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'dl', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'fr',\n",
    "       'type': {'type': 'array',\n",
    "        'elementType': {'type': 'struct',\n",
    "         'fields': [{'name': 'n',\n",
    "           'type': 'string',\n",
    "           'nullable': True,\n",
    "           'metadata': {}},\n",
    "          {'name': 'oa', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "          {'name': 'op', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "          {'name': 'ra', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "          {'name': 'rp', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "        'containsNull': True},\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'n', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'oa', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'oaz', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'od', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'oo', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'op', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'ope', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'opz', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'p', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'pr', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'ra', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'rp', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'sep', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'sub', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "    'containsNull': True},\n",
    "   'nullable': True,\n",
    "   'metadata': {}}]}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract the `treni` data\n",
    "There are multiple information in a json dump. We now consider only information about the trains, excluding riassunto e avvisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(os.path.join(DATA_FOLDER, \"dati_02_09_2022.json\"))\n",
    "\n",
    "date = df.select(\"giorno\")\n",
    "date = date.select(\"giorno\").first().asDict()[\"giorno\"]\n",
    "print(date)\n",
    "\n",
    "day, month, year = date.split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the trains information\n",
    "trains = df.select(\"treni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first row in treni and extract dataframe using the specified schema in `schema`\n",
    "trains1 = trains.select(explode(\"treni\").alias(\"treni\"))\n",
    "\n",
    "# extract the struct of each row\n",
    "trains2 = trains1.select(\"treni.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show list of columns\n",
    "print(list(trains2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapper = {\n",
    "    \"a\": \"train_arrival_stop_name\", \n",
    "    \"c\": \"train_class\",\n",
    "    \"cn\" : \"train_cn\", # unknown\n",
    "    \"dl\": \"train_dl\", # unknown\n",
    "    \"fr\": \"stops\",\n",
    "    \"n\" : \"train_number\",\n",
    "    \"oa\": \"train_arrival_time\",\n",
    "    \"oaz\": \"train_oaz\", # unknown\n",
    "    \"od\": \"train_od\", # unknown\n",
    "    \"oo\" : \"train_oo\", # unknown\n",
    "    \"op\": \"train_departure_time\",\n",
    "    \"ope\" : \"train_ope\", # unknown\n",
    "    \"opz\" : \"train_opz\", # unknown\n",
    "    \"p\" : \"train_departure_stop_name\",\n",
    "    \"pr\": \"train_pr\", # unknown\n",
    "    \"ra\": \"train_arrival_delay\",\n",
    "    \"rp\" : \"train_departure_delay\",\n",
    "    \"sep\" : \"train_sep\", # unknown\n",
    "    \"sub\" : \"train_sub\" # unknown\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all columns\n",
    "trains3 = trains2\n",
    "for k, v in columns_mapper.items():\n",
    "    trains3 = trains3.withColumnRenamed(k, v)\n",
    "\n",
    "trains3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_columns_mapper = {\n",
    "    \"n\": \"stop_name\",\n",
    "    \"oa\": \"stop_arrival_time\",\n",
    "    \"op\": \"stop_departure_time\",\n",
    "    \"ra\": \"stop_arrival_delay\",\n",
    "    \"rp\": \"stop_departure_delay\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stops are stored in an array of structs ([{...}, {...}, ...]). \n",
    "# by using explode, we can extract the struct and create a new row for each stop\n",
    "# we then drop the original array of structs `stops` and we extract from the structs {...} the columns by using \"stops_extracted.*\"\n",
    "# we then drop the original struct and we rename the columns using the mapper \"stops_columns_mapper\n",
    "trains4 = trains3.select(\"*\", explode(\"stops\").alias(\"stops_extracted\")) \\\n",
    "    .drop(\"stops\") \\\n",
    "    .select(\"*\", \"stops_extracted.*\") \\\n",
    "    .drop(\"stops_extracted\")\n",
    "\n",
    "for k, v in stops_columns_mapper.items():\n",
    "    trains4 = trains4.withColumnRenamed(k, v)\n",
    "\n",
    "trains4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trains5 = trains4.withColumn(\"day\", lit(day)).withColumn(\"month\", lit(month)).withColumn(\"year\", lit(year)).withColumn(\"date\", lit(date))\n",
    "\n",
    "# convert day to date format \n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "trains6 = trains5.withColumn(\"date\", to_date(\"date\", \"dd/MM/yyyy\"))\n",
    "trains6.select(col(\"day\"), col(\"month\"), col(\"year\"), col(\"date\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all done! now this should be done for all the files in the folder and then we can save the data in parquet format\n",
    "# we can then use the parquet files to do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_day = day.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the new dataframe in parquet format\n",
    "trains6.write.parquet(os.path.join(DATA_FOLDER, \"parquet\", new_day + \".parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet file\n",
    "trains7 = spark.read.parquet(os.path.join(DATA_FOLDER, \"parquet\", new_day + \".parquet\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a big dataframe containing all the daily datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the code of point 1, we create a big dataframe combining all the daily datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: read all the datasets, extract the `treni` data, add a column with the date for each one of them, and then apply the other transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_daily_dataset(file) -> pyspark.sql.DataFrame: \n",
    "    \"\"\"Here we just add the date column and we explore \"treni\" column\"\"\"\n",
    "    df = spark.read.json(os.path.join(DATA_FOLDER, file), multiLine=True)\n",
    "    date = df.select(\"giorno\")\n",
    "    date = date.select(\"giorno\").first().asDict()[\"giorno\"]\n",
    "    day, month, year = date.split(\"/\")\n",
    "\n",
    "    df = df.select(\"treni\").select(explode(\"treni\").alias(\"treni\")).select(\"treni.*\")\n",
    "    df = df.withColumn(\"day\", lit(day)).withColumn(\"month\", lit(month)).withColumn(\"year\", lit(year)).withColumn(\"date\", lit(date))\n",
    "    df = df.withColumn(\"date\", to_date(\"date\", \"dd/MM/yyyy\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_daily_datasets() -> pyspark.sql.DataFrame: \n",
    "    \"\"\"Reads all daily datasets and union them after applying daily preprocessing\"\"\"\n",
    "    all_files = os.listdir(DATA_FOLDER)\n",
    "    all_files = [f for f in all_files if f.endswith(\".json\")]\n",
    "    all_files.sort()\n",
    "    df = read_preprocess_daily_dataset(all_files[0])\n",
    "    for f in tqdm(all_files[1:]):\n",
    "        df = df.unionByName(read_preprocess_daily_dataset(f), allowMissingColumns=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_file_name(file_name: str) -> str:\n",
    "    \"\"\"Extracts the date from the file name\"\"\"\n",
    "    date = file_name.split(\"_\")[1:]\n",
    "    date = \"_\".join(date)\n",
    "    date = date.split(\".\")[0]\n",
    "    # convert to yyyy-mm-dd format\n",
    "    day, month, year = date.split(\"_\")\n",
    "    date = \"{}-{}-{}\".format(year, month, day)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_available_files() -> List[str]:\n",
    "    \"\"\"Returns the list of files that are available in the data folder\"\"\"\n",
    "    all_files = os.listdir(DATA_FOLDER)\n",
    "    all_files = [f for f in all_files if f.endswith(\".json\")]\n",
    "    all_files.sort(key= lambda x: get_date_from_file_name(x))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 90 files in the data folder\n",
      "The first file is dati_01_01_2023.json\n",
      "The last file is dati_31_03_2023.json\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} files in the data folder\".format(len(get_available_files())))\n",
    "print(\"The first file is {}\".format(get_available_files()[0]))\n",
    "print(\"The last file is {}\".format(get_available_files()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [01:16<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "df = read_all_daily_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame: \n",
    "    \"\"\"Procesess the entire dataset of multiple days\"\"\"\n",
    "\n",
    "    columns_mapper = {\n",
    "        \"a\": \"train_arrival_stop_name\", \n",
    "        \"c\": \"train_class\",\n",
    "        \"cn\" : \"train_cn\", # unknown\n",
    "        \"dl\": \"train_dl\", # unknown\n",
    "        \"fr\": \"stops\",\n",
    "        \"n\" : \"train_number\",\n",
    "        \"oa\": \"train_arrival_time\",\n",
    "        \"oaz\": \"train_oaz\", # unknown\n",
    "        \"od\": \"train_od\", # unknown\n",
    "        \"oo\" : \"train_oo\", # unknown\n",
    "        \"op\": \"train_departure_time\",\n",
    "        \"ope\" : \"train_ope\", # unknown\n",
    "        \"opz\" : \"train_opz\", # unknown\n",
    "        \"p\" : \"train_departure_stop_name\",\n",
    "        \"pr\": \"train_pr\", # unknown\n",
    "        \"ra\": \"train_arrival_delay\",\n",
    "        \"rp\" : \"train_departure_delay\",\n",
    "        \"sep\" : \"train_sep\", # unknown\n",
    "        \"sub\" : \"train_sub\" # unknown\n",
    "    }\n",
    "\n",
    "    # rename all columns\n",
    "    for k, v in columns_mapper.items():\n",
    "        df = df.withColumnRenamed(k, v)\n",
    "\n",
    "    stops_columns_mapper = {\n",
    "        \"n\": \"stop_name\",\n",
    "        \"oa\": \"stop_arrival_time\",\n",
    "        \"op\": \"stop_departure_time\",\n",
    "        \"ra\": \"stop_arrival_delay\",\n",
    "        \"rp\": \"stop_departure_delay\"\n",
    "    }\n",
    "\n",
    "    df = df.select(\"*\", explode(\"stops\").alias(\"stops_extracted\")) \\\n",
    "    .drop(\"stops\") \\\n",
    "    .select(\"*\", \"stops_extracted.*\") \\\n",
    "    .drop(\"stops_extracted\")\n",
    "\n",
    "    for k, v in stops_columns_mapper.items():\n",
    "        df = df.withColumnRenamed(k, v)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = preprocess_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 20:24:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/04/22 20:24:44 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# store in columnar format\n",
    "df2.write.parquet(os.path.join(DATA_FOLDER, \"parquet\", \"all.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrenitaliaSpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbad97b5d24366a386ca7fadb2084e585e66698cec63432bb0202e3b6a0edfdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
