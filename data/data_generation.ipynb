{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to generate some of the files used by our project website. \n",
    "\n",
    "Data is taken from [OpenStats](https://openstats.altervista.org) and has to be pre-processed in the [Data Wrangling Notebook](../exploratory_data_analysis/data_wrangling.ipynb). This notebook assumes you have already pre-preprocessed the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Initial Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import lit, col, to_date\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import stddev\n",
    "from pyspark.sql.functions import count, countDistinct, concat, sum\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"...\" # replace with the output folder of the data_wrangling script\n",
    "file = \"all.parquet\"\n",
    "\n",
    "SAVE_COMPUTATIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/31 16:31:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Trenitalia\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# set driver memory to 4GB\n",
    "spark.sparkContext._conf.setAll([('spark.driver.memory', '4g')])\n",
    "\n",
    "# get sc \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 8316723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(os.path.join(DATA_FOLDER, \"parquet\", file))\n",
    "print(\"Number of rows: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset of the stops\n",
    "stops = spark.read.csv(os.path.join(DATA_FOLDER, \"stops.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "stops_column_renamer = {\n",
    "    \"name\": \"stop_name\",\n",
    "    \"lat\": \"stop_lat\",\n",
    "    \"lon\": \"stop_lon\",\n",
    "    \"station_id\": \"stop_id\", \n",
    "    \"name_short\": \"stop_name_short\",\n",
    "    \"id_region\": \"stop_id_region\",\n",
    "}\n",
    "\n",
    "for k, v in stops_column_renamer.items():\n",
    "    stops = stops.withColumnRenamed(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days:  90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date:  2023-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last date:  2023-03-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trains:  11496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops:  2291\n",
      "Number of train classes:  11\n"
     ]
    }
   ],
   "source": [
    "# number of days\n",
    "print(\"Number of days: \", df.select(\"date\").distinct().count())\n",
    "\n",
    "# first date\n",
    "print(\"First date: \", df.select(\"date\").distinct().orderBy(\"date\").first().asDict()[\"date\"])\n",
    "\n",
    "# last date\n",
    "print(\"Last date: \", df.select(\"date\").distinct().orderBy(\"date\", ascending=False).first().asDict()[\"date\"])\n",
    "\n",
    "# number of trains\n",
    "print(\"Number of trains: \", df.select(\"train_number\").distinct().count())\n",
    "\n",
    "# number of stops\n",
    "print(\"Number of stops: \", df.select(\"stop_name\").distinct().count())\n",
    "\n",
    "# number of train classes\n",
    "print(\"Number of train classes: \", df.select(\"train_class\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_arrival_stop_name', 'train_class', 'train_cn', 'train_dl', 'train_number', 'train_arrival_time', 'oae', 'train_oaz', 'train_od', 'train_oo', 'train_departure_time', 'train_ope', 'train_opz', 'train_departure_stop_name', 'train_pr', 'train_arrival_delay', 'train_departure_delay', 'sea', 'train_sep', 'train_sub', 'day', 'month', 'year', 'date', 'stop_name', 'stop_arrival_time', 'stop_departure_time', 'stop_arrival_delay', 'stop_departure_delay']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preprocessing\n",
    "As step of preprocessing, we remove all delays that are anomalous, i.e. they are not in the range [-100, 300] minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column oae: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_oaz: 106676\n",
      "Distinct values for column train_od: 289\n",
      "Distinct values for column train_oo: 300\n",
      "Distinct values for column train_cn: 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_dl: 1756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_ope: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_opz: 100309\n",
      "Distinct values for column train_pr: 1\n",
      "Distinct values for column sea: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_sep: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_sub: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "interesting_columns = ['oae', 'train_oaz', \"train_od\", \"train_oo\", \"train_cn\", \"train_dl\", \"train_ope\", \"train_opz\", \"train_pr\", \"sea\", \"train_sep\", \"train_sub\" ]\n",
    "\n",
    "# for each of the interesting_columns compute and show the distinct values\n",
    "for c in interesting_columns:\n",
    "    print(\"Distinct values for column {}: {}\".format(c, df.select(c).distinct().count()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all values of delays that are not in the range [-100, 300] if they are numerical\n",
    "MIN_DELAY = -100\n",
    "MAX_DELAY = 300\n",
    "df = df.filter((col(\"stop_arrival_delay\").cast(\"double\").isNull()) | (col(\"stop_arrival_delay\").cast(\"double\") >= MIN_DELAY) & (col(\"stop_arrival_delay\").cast(\"double\") <= MAX_DELAY))\n",
    "\n",
    "# for trains that have a non-null `train_sub`, we replace `train_class` with `train_sub`\n",
    "# NOTE: `train_sub` contains the category of high speed train (e.g. \"FR\", \"FB\", \"FA\" which stand for Frecciarossa, Frecciabianca, Frecciargento)\n",
    "# When a train is high speed, `train_class` is empty, apart from one case, which is the FrecciaRossa Milano -> Paris where the train_class is EC (EuroCity)\n",
    "# We replace that with FR (FrecciaRossa)\n",
    "df = df.withColumn(\"train_class\", F.when(col(\"train_sub\").isNotNull(), col(\"train_sub\")).otherwise(col(\"train_class\")))\n",
    "\n",
    "# remove all the trains that are not in the following classes\n",
    "KEEP_TRAIN_CLASSES = [\"IC\", \"ICN\", \"REG\", \"\", \"EC\", \"FA\", \"FB\", \"FR\"]\n",
    "\n",
    "df = df.filter(col(\"train_class\").isin(KEEP_TRAIN_CLASSES))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Statistics for each station"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each distinct station, we want to obtain: \n",
    "1. Station name\n",
    "2. Latitude, longitude\n",
    "3. Average arrival delay\n",
    "4. Median arrival delay\n",
    "5. % of trains with delay > 3\n",
    "6. % of trains with delay > 5\n",
    "7. % of trains with delay > 10\n",
    "8. Number of distinct train numbers that stopped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|        stop_name|count_stops|\n",
      "+-----------------+-----------+\n",
      "|TERONTOLA CORTONA|       6397|\n",
      "| PRATO BORGONUOVO|       3370|\n",
      "|           ZOAGLI|       3126|\n",
      "|  ROCCA D`EVANDRO|       1683|\n",
      "|     BAGNACAVALLO|       3062|\n",
      "+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create a dataframe that counts the number of trains passed by each stop\n",
    "stop_counts = df.groupBy(\"stop_name\").agg(F.count(\"train_number\").alias(\"count_stops\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column True if train had > 3 minutes of delay\n",
    "data_stop = df.join(stops, on=\"stop_name\", how=\"inner\")\n",
    "\n",
    "data_stop = data_stop \\\n",
    "    .filter(col(\"stop_arrival_delay\").cast(\"double\").isNotNull()) \\\n",
    "    .withColumn(\"stop_arrival_delay_double\", col(\"stop_arrival_delay\").cast(\"double\")) \\\n",
    "    .drop(\"stop_arrival_delay\") \\\n",
    "    .withColumnRenamed(\"stop_arrival_delay_double\", \"stop_arrival_delay\") \\\n",
    "    .withColumn(\"3m_delay\", col(\"stop_arrival_delay\") > 3)\\\n",
    "    .withColumn(\"5m_delay\", col(\"stop_arrival_delay\") > 5)\\\n",
    "    .withColumn(\"10m_delay\", col(\"stop_arrival_delay\") > 10)\\\n",
    "    .withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\"))\\\n",
    "    .withColumn(\"train_id\", concat(col(\"train_class\"), col(\"train_number\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stop_stat = data_stop.groupBy(\"stop_name\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        # F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=\"stop_name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops:  2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of stops: \", data_stop_stat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops in (lat,long) dataset:  2961\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stops in (lat,long) dataset: \", stops.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops in final dataset:  2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of stops in final dataset: \", data_stop_stat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_THRESHOLD_STOP = 12\n",
    "# we keep only stops that had at least one train per week, over the 3 months analyzed\n",
    "data_stop_stat = data_stop_stat.filter(col(\"count_trains\") >= MIN_THRESHOLD_STOP)\n",
    "\n",
    "# we filter out stops without lat/lon\n",
    "data_stop_stat = data_stop_stat.filter(col(\"stop_lat\").isNotNull() & col(\"stop_lon\").isNotNull())\n",
    "\n",
    "data_stop_stat = data_stop_stat.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>avg_arrival_delay</th>\n",
       "      <th>median_arrival_delay</th>\n",
       "      <th>count_trains</th>\n",
       "      <th>count_3m_delay</th>\n",
       "      <th>count_5m_delay</th>\n",
       "      <th>count_10m_delay</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>count_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROCCA D`EVANDRO</td>\n",
       "      <td>3.816927</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29</td>\n",
       "      <td>630</td>\n",
       "      <td>393</td>\n",
       "      <td>145</td>\n",
       "      <td>41.438861</td>\n",
       "      <td>13.911338</td>\n",
       "      <td>1683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TERONTOLA CORTONA</td>\n",
       "      <td>4.113270</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2319</td>\n",
       "      <td>1344</td>\n",
       "      <td>494</td>\n",
       "      <td>43.210263</td>\n",
       "      <td>12.007906</td>\n",
       "      <td>6397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MONTIRONE</td>\n",
       "      <td>3.896465</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24</td>\n",
       "      <td>695</td>\n",
       "      <td>446</td>\n",
       "      <td>200</td>\n",
       "      <td>45.451352</td>\n",
       "      <td>10.239482</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VIGNA DI VALLE</td>\n",
       "      <td>3.741001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31</td>\n",
       "      <td>905</td>\n",
       "      <td>300</td>\n",
       "      <td>32</td>\n",
       "      <td>42.076863</td>\n",
       "      <td>12.210716</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAGNACAVALLO</td>\n",
       "      <td>4.731275</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1413</td>\n",
       "      <td>864</td>\n",
       "      <td>304</td>\n",
       "      <td>44.412138</td>\n",
       "      <td>11.969398</td>\n",
       "      <td>3062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PONT SAINT MARTIN</td>\n",
       "      <td>2.061974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82</td>\n",
       "      <td>806</td>\n",
       "      <td>457</td>\n",
       "      <td>152</td>\n",
       "      <td>45.589834</td>\n",
       "      <td>7.795024</td>\n",
       "      <td>3215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>S. AMBROGIO</td>\n",
       "      <td>2.307010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "      <td>552</td>\n",
       "      <td>272</td>\n",
       "      <td>71</td>\n",
       "      <td>45.101161</td>\n",
       "      <td>7.360933</td>\n",
       "      <td>3391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>S. DONA` DI PIAVE</td>\n",
       "      <td>2.190600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87</td>\n",
       "      <td>1292</td>\n",
       "      <td>629</td>\n",
       "      <td>202</td>\n",
       "      <td>45.639830</td>\n",
       "      <td>12.559791</td>\n",
       "      <td>6547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FILATTIERA</td>\n",
       "      <td>2.124722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>590</td>\n",
       "      <td>356</td>\n",
       "      <td>86</td>\n",
       "      <td>44.331370</td>\n",
       "      <td>9.933352</td>\n",
       "      <td>2253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROBILANTE</td>\n",
       "      <td>0.226592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "      <td>94</td>\n",
       "      <td>39</td>\n",
       "      <td>16</td>\n",
       "      <td>44.296496</td>\n",
       "      <td>7.510906</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stop_name  avg_arrival_delay  median_arrival_delay  count_trains  \\\n",
       "0     ROCCA D`EVANDRO           3.816927                   3.0            29   \n",
       "1   TERONTOLA CORTONA           4.113270                   2.0            95   \n",
       "2           MONTIRONE           3.896465                   2.0            24   \n",
       "3      VIGNA DI VALLE           3.741001                   3.0            31   \n",
       "4        BAGNACAVALLO           4.731275                   3.0            39   \n",
       "..                ...                ...                   ...           ...   \n",
       "95  PONT SAINT MARTIN           2.061974                   1.0            82   \n",
       "96        S. AMBROGIO           2.307010                   1.0            46   \n",
       "97  S. DONA` DI PIAVE           2.190600                   1.0            87   \n",
       "98         FILATTIERA           2.124722                   1.0            39   \n",
       "99          ROBILANTE           0.226592                   0.0            36   \n",
       "\n",
       "    count_3m_delay  count_5m_delay  count_10m_delay   stop_lat   stop_lon  \\\n",
       "0              630             393              145  41.438861  13.911338   \n",
       "1             2319            1344              494  43.210263  12.007906   \n",
       "2              695             446              200  45.451352  10.239482   \n",
       "3              905             300               32  42.076863  12.210716   \n",
       "4             1413             864              304  44.412138  11.969398   \n",
       "..             ...             ...              ...        ...        ...   \n",
       "95             806             457              152  45.589834   7.795024   \n",
       "96             552             272               71  45.101161   7.360933   \n",
       "97            1292             629              202  45.639830  12.559791   \n",
       "98             590             356               86  44.331370   9.933352   \n",
       "99              94              39               16  44.296496   7.510906   \n",
       "\n",
       "    count_stops  \n",
       "0          1683  \n",
       "1          6397  \n",
       "2          1987  \n",
       "3          2304  \n",
       "4          3062  \n",
       "..          ...  \n",
       "95         3215  \n",
       "96         3391  \n",
       "97         6547  \n",
       "98         2253  \n",
       "99         1607  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stop_pandas = data_stop_stat.toPandas()\n",
    "data_stop_pandas.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "if SAVE_COMPUTATIONS:\n",
    "    data_stop_pandas.to_csv((\"dataset_generated/data_stop/data_stop.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Statistics for each day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/30 16:34:16 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "stop_counts = df.withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\")).groupBy(\"stop_name\", \"day_of_week\").agg(F.count(\"train_number\").alias(\"count_stops\")).cache()\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name\", \"day_of_week\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        # F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=[\"stop_name\", \"day_of_week\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_stop_stat_pandas = data_stop_stat.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file for each day of week with the statistics\n",
    "if SAVE_COMPUTATIONS:\n",
    "    for day in data_stop_stat_pandas[\"day_of_week\"].unique():\n",
    "        data_stop_stat_pandas[data_stop_stat_pandas[\"day_of_week\"] == day].to_csv((\"dataset_generated/data_stop/data_stop_{}.csv\".format(day)), index=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Statistics for each train type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/30 16:34:44 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "stop_counts = df.groupBy(\"stop_name\", \"train_class\").agg(F.count(\"train_number\").alias(\"count_stops\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stop_stat = data_stop.groupBy(\"stop_name\", \"train_class\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=[\"stop_name\", \"train_class\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_stop_stat_pandas = data_stop_stat.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file for each day of week with the statistics\n",
    "if SAVE_COMPUTATIONS:\n",
    "    for train_class in data_stop_stat_pandas[\"train_class\"].unique():\n",
    "        data_stop_stat_pandas[data_stop_stat_pandas[\"train_class\"] == train_class].to_csv((\"dataset_generated/data_stop/data_stop_class_{}.csv\".format(train_class)), index=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Statistics for each week day and train type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = df.withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\")).groupBy(\"stop_name\", \"train_class\", \"day_of_week\").agg(F.count(\"train_number\").alias(\"count_stops\"))\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name\", \"train_class\", \"day_of_week\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        # F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=[\"stop_name\", \"train_class\", \"day_of_week\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_stop_stat_pandas = data_stop_stat.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each combination of weekday and train_class, create a file with the statistics\n",
    "if SAVE_COMPUTATIONS:\n",
    "    for day in data_stop_stat_pandas[\"day_of_week\"].unique():\n",
    "        for train_class in data_stop_stat_pandas[\"train_class\"].unique():\n",
    "            data_stop_stat_pandas[(data_stop_stat_pandas[\"day_of_week\"] == day) & (data_stop_stat_pandas[\"train_class\"] == train_class)].to_csv((\"dataset_generated/data_stop/data_stop_mix_{}_{}.csv\".format(day, train_class)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if SAVE_COMPUTATIONS: \n",
    "    # zip the folder\n",
    "    shutil.make_archive(\"dataset_generated/data_stop\", 'zip', \"dataset_generated/data_stop\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1d. Statistics for each region\n",
    "\n",
    "Here we aim to extract some insights on train delays for each Italian region. Specifically, we want to compute the average arrival delay for each region, and the statistics used in the previous tasks. We are interested to see those differences for each train class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapper = {\n",
    "    1: \"Lombardia\",\n",
    "    2: \"Liguria\",\n",
    "    3: \"Piemonte\",\n",
    "    4: \"Valle d'Aosta\",\n",
    "    5: \"Lazio\",\n",
    "    6: \"Umbria\",\n",
    "    7: \"Molise\",\n",
    "    8: \"Emilia-Romagna\",\n",
    "    9: \"Trentino-Alto Adige\",\n",
    "    10: \"Friuli-Venezia Giulia\",\n",
    "    11: \"Marche\",\n",
    "    12 : \"Veneto\",\n",
    "    13: \"Toscana\",\n",
    "    14: \"Sicilia\",\n",
    "    15: \"Basilicata\",\n",
    "    16: \"Puglia\",\n",
    "    17: \"Calabria\",\n",
    "    18: \"Campania\",\n",
    "    19: \"Abruzzo\",\n",
    "    20: \"Sardegna\",\n",
    "    21: \"Trentino-Alto Adige\", #actually some stops of province of Trento.....\n",
    "    22: \"Trentino-Alto Adige\", #actually some stops of province of Bolzano.....\n",
    "    None: \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "stops_region = stops.withColumn(\"stop_name_region\", F.udf(lambda x: region_mapper[x], StringType())(\"stop_id_region\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stop_first = df.join(stops_region, on=\"stop_name\", how=\"inner\")\n",
    "\n",
    "data_stop = data_stop_first \\\n",
    "    .filter(col(\"stop_arrival_delay\").cast(\"double\").isNotNull()) \\\n",
    "    .withColumn(\"stop_arrival_delay_double\", col(\"stop_arrival_delay\").cast(\"double\")) \\\n",
    "    .drop(\"stop_arrival_delay\") \\\n",
    "    .withColumnRenamed(\"stop_arrival_delay_double\", \"stop_arrival_delay\") \\\n",
    "    .withColumn(\"3m_delay\", col(\"stop_arrival_delay\") > 3)\\\n",
    "    .withColumn(\"5m_delay\", col(\"stop_arrival_delay\") > 5)\\\n",
    "    .withColumn(\"10m_delay\", col(\"stop_arrival_delay\") > 10)\\\n",
    "    .withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\"))\\\n",
    "    .withColumn(\"train_id\", concat(col(\"train_class\"), col(\"train_number\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = data_stop.groupBy(\"stop_name_region\").agg(F.count(\"train_id\").alias(\"count_stops\"))\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name_region\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        F.countDistinct(\"stop_name\").alias(\"count_stations\"),\n",
    "        F.first(\"stop_id_region\").alias(\"stop_id_region\"),\n",
    "    ).join(stop_counts, on=[\"stop_name_region\"], how=\"inner\").sort(\"stop_id_region\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if SAVE_COMPUTATIONS:\n",
    "    data_stop_stat.toPandas().to_csv(\"dataset_generated/data_stop_region/data_stop_region.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = data_stop.groupBy(\"stop_name_region\", \"train_class\").agg(F.count(\"train_id\").alias(\"count_stops\"))\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name_region\", \"train_class\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        F.countDistinct(\"stop_name\").alias(\"count_stations\"),\n",
    "        F.first(\"stop_id_region\").alias(\"stop_id_region\"),\n",
    "    ).join(stop_counts, on=[\"stop_name_region\", \"train_class\"], how=\"inner\").sort(\"stop_id_region\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------------+--------------------+------------+--------------+--------------+-----------+\n",
      "|stop_name_region|train_class| avg_arrival_delay|median_arrival_delay|count_trains|count_stations|stop_id_region|count_stops|\n",
      "+----------------+-----------+------------------+--------------------+------------+--------------+--------------+-----------+\n",
      "|                |         IC| 7.308510638297872|                 3.0|          13|             3|          null|         94|\n",
      "|                |        REG|1.4068916921810597|                 1.0|        2811|           154|          null|     320734|\n",
      "|                |         FR|  5.76683697166744|                 3.0|          72|             3|          null|       4306|\n",
      "|                |           |             -13.0|               -13.0|           1|             1|          null|          1|\n",
      "|                |         FA| 6.469135802469136|                 4.0|          10|             1|          null|        729|\n",
      "|       Lombardia|         FA|-4.471794871794872|                -3.0|           2|             4|           1.0|        195|\n",
      "|       Lombardia|         EC|  4.68080888183981|                 3.0|          35|            11|           1.0|       2522|\n",
      "|       Lombardia|         FR| 4.848558437532293|                 2.0|         138|            10|           1.0|      19354|\n",
      "|       Lombardia|        ICN| 5.670542635658915|                 1.0|           8|             4|           1.0|        516|\n",
      "|       Lombardia|        REG| 3.090178510900942|                 2.0|        2786|           289|           1.0|    1268606|\n",
      "+----------------+-----------+------------------+--------------------+------------+--------------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_stop_stat.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if SAVE_COMPUTATIONS:\n",
    "    data_stop_stat_pandas = data_stop_stat.toPandas()\n",
    "\n",
    "    for train_class in data_stop_stat_pandas[\"train_class\"].unique():\n",
    "        data_stop_stat_pandas[data_stop_stat_pandas[\"train_class\"] == train_class].to_csv((\"dataset_generated/data_stop_region/data_stop_region_class_{}.csv\".format(train_class)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the folder\n",
    "if SAVE_COMPUTATIONS:\n",
    "    shutil.make_archive(\"dataset_generated/data_stop_region\", 'zip', \"dataset_generated/data_stop_region\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train statistics\n",
    "For each train, the goal is to store the following information:\n",
    "1. Ordered list of the stations it stopped at\n",
    "2. For each destination: \n",
    "    1. Average arrival delay\n",
    "    2. Median arrival delay\n",
    "    3. % of trains with delay > 3\n",
    "    4. % of trains with delay > 5\n",
    "    5. % of trains with delay > 10\n",
    "    6. Number of days it stopped at the destination\n",
    "\n",
    "To avoid keeping statistics for temporary stops and trains, we remove: \n",
    "- Trains that appeared in the dataset only <10  distinct times\n",
    "- Stops for a train, that appeared less than 20% of the dates that the train appeared in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we add coordinates to each stop, so that we can drop stops for which we don't have the coordinates\n",
    "df_trains = df.join(stops, on=\"stop_name\", how=\"inner\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. For each train, we obtain the statistics of each of its stops, removing the stops that appear less than 20% of the dates that the train appeared in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, we have to keep in mind that the same train number can refer to multiple trains, so we also have to group by the destination stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|train_arrival_stop_name|\n",
      "+-----------------------+\n",
      "|           ROMA TERMINI|\n",
      "|         COMO NORD LAGO|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# this shows that there's multiple trains with the same number\n",
    "df \\\n",
    "    .filter((F.col(\"train_class\") == \"REG\") & (F.col(\"train_number\") == \"4113\"))\\\n",
    "    .select(\"train_arrival_stop_name\")\\\n",
    "    .distinct()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we create a new column with the delay in minutes. For all stops this has to be the arrival_delay apart for the first stop of each train, for which it has to be the departure_delay\n",
    "df_trains1 = df_trains.withColumn(\"delay\", F.when(F.col(\"stop_arrival_delay\") == \"N\", F.col(\"stop_departure_delay\")).otherwise(F.col(\"stop_arrival_delay\")))\n",
    "\n",
    "# second, add a column \"stop_arrival_time\", which is the arrival time at the stop, in the format \"HH:MM\"\n",
    "# if the stop is the first stop of the train, then the stop_arrival_time is the departure time of the train\n",
    "\n",
    "df_trains1 = df_trains1\\\n",
    "    .withColumn(\"stop_time\", F.when(F.col(\"stop_arrival_time\") == 0, F.col(\"stop_departure_time\")).otherwise(F.col(\"stop_arrival_time\"))) \\\n",
    "    .withColumn(\"stop_time\", F.date_format(F.col(\"stop_time\").cast(\"timestamp\"), \"HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the delay to double and add a counter if the train was 3m, 5m, 10m late\n",
    "# add a column with the number of distinct dates for each train\n",
    "# remove the column if the delay cannot be casted to double\n",
    "df_trains2 = df_trains1 \\\n",
    "    .withColumn(\"delay\", F.col(\"delay\").cast(\"double\")) \\\n",
    "    .filter(F.col(\"delay\").isNotNull()) \\\n",
    "    .withColumn(\"3m_delay\", F.when(F.col(\"delay\") >= 3, 1).otherwise(0)) \\\n",
    "    .withColumn(\"5m_delay\", F.when(F.col(\"delay\") >= 5, 1).otherwise(0)) \\\n",
    "    .withColumn(\"10m_delay\", F.when(F.col(\"delay\") >= 10, 1).otherwise(0))\\\n",
    "    .select(\"train_departure_stop_name\", \"train_arrival_stop_name\", \"train_class\", \"train_number\", \"date\", \"stop_id\", \"stop_time\", \"stop_name\", \"stop_name_short\", \"stop_lat\", \"stop_lon\", \"stop_id_region\", \"3m_delay\", \"5m_delay\", \"10m_delay\", \"delay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each train, we obtain the statistics of each of its stops, removing the stops that appear less than 20% of the dates that the train appeared in the dataset\n",
    "# add the stop incremental number\n",
    "df_trains_stat1 = df_trains2.groupBy(\"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\", \"stop_name\") \\\n",
    "    .agg(\n",
    "        F.avg(\"delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"date\").alias(\"count_dates_stop\"),\n",
    "        F.min(\"date\").alias(\"first_date\"),\n",
    "        F.max(\"date\").alias(\"last_date\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\"),\n",
    "        F.mode(\"stop_time\").alias(\"stop_time\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts how many distinct dates a train apparead in the dataset\n",
    "# and filter out trains that didn't appear at least 4 times a month (12 times in total)\n",
    "MIN_THRESHOLD_TRAIN = 12\n",
    "\n",
    "df_trains_counts = df_trains2.groupBy(\"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\"date\").alias(\"count_dates_train\")\n",
    "    ) \\\n",
    "    .filter(F.col(\"count_dates_train\") >= MIN_THRESHOLD_TRAIN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store the data we create a mapper from a (train_class, train_number, train_arrival_stop_name) to an id, and we store the information of that train on a file with the name of the id. To do that, we add a column which is a monotonic increasing id, and we use that as the id of that train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trains_counts = df_trains_counts \\\n",
    "    .withColumn(\"train_id\", F.monotonically_increasing_id()) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets and filter out stations that appear in less than 20% of the dates\n",
    "df_trains_stat2 = df_trains_stat1.join(df_trains_counts, on=[\"train_class\", \"train_number\", \"train_arrival_stop_name\", \"train_departure_stop_name\"], how=\"inner\") \\\n",
    "    .filter(F.col(\"count_dates_stop\") >= F.col(\"count_dates_train\") * 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 209:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------------------+-------------------------+------------------+------------------+--------------------+----------------+----------+----------+--------------+--------------+---------------+---------+---------+---------+-----------------+-------------+\n",
      "|train_class|train_number|train_arrival_stop_name|train_departure_stop_name|         stop_name| avg_arrival_delay|median_arrival_delay|count_dates_stop|first_date| last_date|count_3m_delay|count_5m_delay|count_10m_delay| stop_lat| stop_lon|stop_time|count_dates_train|     train_id|\n",
      "+-----------+------------+-----------------------+-------------------------+------------------+------------------+--------------------+----------------+----------+----------+--------------+--------------+---------------+---------+---------+---------+-----------------+-------------+\n",
      "|           |        9505|                SALERNO|          MILANO CENTRALE|            AREZZO| 5.642857142857143|                 3.0|              14|2023-01-09|2023-01-22|             8|             5|              4|43.461725|11.874189|    07:48|               14| 962072674357|\n",
      "|           |        9505|                SALERNO|          MILANO CENTRALE|   NAPOLI CENTRALE| 5.857142857142857|                -1.0|              14|2023-01-09|2023-01-22|             7|             5|              3|40.852933|14.272908|    10:28|               14| 962072674357|\n",
      "|         EC|        1281|    VENEZIA SANTA LUCIA|                 BRENNERO|            TRENTO| 3.235294117647059|                 1.0|              17|2023-01-01|2023-03-26|             5|             2|              1|46.072414|11.118941|    12:02|               17|1047972020245|\n",
      "|         EC|         151|        MILANO CENTRALE|                  CHIASSO|  COMO S. GIOVANNI|1.2159090909090908|                -1.0|              88|2023-01-01|2023-03-31|            18|             8|              3|45.808946| 9.072331|    15:01|               88|1125281431562|\n",
      "|         EC|         307|           BOLOGNA C.LE|                  CHIASSO|      BOLOGNA C.LE| 8.423076923076923|                 3.0|              26|2023-02-19|2023-03-31|            16|             9|              6| 44.50626|11.342267|    12:30|               26| 944892805178|\n",
      "|         EC|         307|           BOLOGNA C.LE|                  CHIASSO|   MILANO ROGOREDO| 4.730769230769231|                 4.0|              26|2023-02-19|2023-03-31|            18|            12|              1|45.433866|   9.2391|    10:20|               26| 944892805178|\n",
      "|         EC|         307|           BOLOGNA C.LE|                  CHIASSO|            MODENA| 5.653846153846154|                 2.0|              26|2023-02-19|2023-03-31|            11|             8|              4|44.654461|10.930373|    12:07|               26| 944892805178|\n",
      "|         EC|         311|    VENEZIA SANTA LUCIA|                  CHIASSO|   MILANO CENTRALE|               2.0|                -1.0|              22|2023-02-24|2023-03-31|             6|             5|              3|45.486347| 9.204528|    11:50|               22|1546188226597|\n",
      "|         EC|         311|    VENEZIA SANTA LUCIA|                  CHIASSO|    VENEZIA MESTRE| 5.090909090909091|                 2.0|              22|2023-02-24|2023-03-31|             9|             6|              5|45.482123|12.232069|    14:30|               22|1546188226597|\n",
      "|         EC|         311|    VENEZIA SANTA LUCIA|                  CHIASSO|VERONA PORTA NUOVA|7.7727272727272725|                 3.0|              22|2023-02-24|2023-03-31|            12|             9|              7|45.428603|10.982377|    13:28|               22|1546188226597|\n",
      "+-----------+------------+-----------------------+-------------------------+------------------+------------------+--------------------+----------------+----------+----------+--------------+--------------+---------------+---------+---------+---------+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trains_stat2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 16:53:56 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# add the stop incremental number\n",
    "df_trains_stat3 = df_trains_stat2 \\\n",
    "    .withColumn(\"stop_number\", F.row_number().over(Window.partitionBy(\"train_id\").orderBy(\"stop_time\"))) \\\n",
    "    .sort(\"train_id\", \"stop_number\") \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 16:53:56 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# remove trains without class and order by \"count_dates_train\"\n",
    "df_trains_stat3 = df_trains_stat3.filter(F.col(\"train_class\")!= \"\") \\\n",
    "    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now we store df_trains_stat2 in a csv file, partitioned by the column \"train_id\". \n",
    "# Each \"train_id\" should have a single file, and each file should contain the statistics of all the stops of the train\n",
    "\n",
    "# remove folder if it already exists\n",
    "\n",
    "if SAVE_COMPUTATIONS : \n",
    "    import shutil\n",
    "    shutil.rmtree(\"dataset_generated/data_train_stat\")\n",
    "\n",
    "\n",
    "    df_trains_stat3 \\\n",
    "        .repartition(\"train_id\") \\\n",
    "        .write \\\n",
    "        .partitionBy(\"train_id\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"dataset_generated/data_train_stat/data_train_stat.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_COMPUTATIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the previous cell generates a folder with a lot of subfolders, one for each train. \n",
    "# each folder name is called \"train_id=XXXXX\", where XXXXX is the id of the train\n",
    "# we want to generate a new folder with a single file for each train, where the file name is the train id\n",
    "# the file should contain the statistics of all the stops of the train\n",
    "\n",
    "# remove folder if it already exists\n",
    "if SAVE_COMPUTATIONS :\n",
    "    import shutil\n",
    "\n",
    "    import os\n",
    "\n",
    "    if os.path.exists(\"dataset_generated/data_train_stat_single_file\") :\n",
    "        shutil.rmtree(\"dataset_generated/data_train_stat_single_file\")\n",
    "\n",
    "    os.mkdir(\"dataset_generated/data_train_stat_single_file\")\n",
    "\n",
    "    import glob\n",
    "    import shutil\n",
    "\n",
    "    # get all the subfolders\n",
    "    subfolders = [f.path for f in os.scandir(\"dataset_generated/data_train_stat/data_train_stat.csv\") if f.is_dir() ] \n",
    "\n",
    "    # for each subfolder, get the csv file and rename it\n",
    "    for subfolder in subfolders : \n",
    "        # get the csv file\n",
    "        csv_file = glob.glob(subfolder + \"/*.csv\")[0]\n",
    "\n",
    "        # get the train id\n",
    "        train_id = subfolder.split(\"=\")[1]\n",
    "\n",
    "        # rename the file\n",
    "        shutil.copy(csv_file, \"dataset_generated/data_train_stat_single_file/\" + train_id + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we zip the folder data_train_stat_single_file and we put the zip in data_train_stat\n",
    "if SAVE_COMPUTATIONS :\n",
    "    import shutil\n",
    "    shutil.make_archive(\"dataset_generated/data_train_stat/data_train_stat\", 'zip', \"dataset_generated/data_train_stat_single_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "# df_trains_stat3 = spark.read.option(\"header\", \"true\").csv(\"dataset_generated/data_train_stat/data_train_stat.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to elaborate some general stastistics about a train, and store a dataset which is an index of the train ids, and the statistics of that train.\n",
    "\n",
    "The aggregated statistics that we want to store are: \n",
    "1. Average arrival delay at each destination\n",
    "2. Median arrival delay at each destination\n",
    "3. % of trains with delay > 3 at each destination\n",
    "4. % of trains with delay > 5 at each destination\n",
    "5. % of trains with delay > 10 at each destination\n",
    "6. Number of days the train ran\n",
    "7. Number of stops of the train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------------------+-------------------------+---------+------------------+--------------------+----------------+----------+----------+--------------+--------------+---------------+---------+--------+---------+-----------------+--------+-----------+\n",
      "|train_class|train_number|train_arrival_stop_name|train_departure_stop_name|stop_name| avg_arrival_delay|median_arrival_delay|count_dates_stop|first_date| last_date|count_3m_delay|count_5m_delay|count_10m_delay| stop_lat|stop_lon|stop_time|count_dates_train|train_id|stop_number|\n",
      "+-----------+------------+-----------------------+-------------------------+---------+------------------+--------------------+----------------+----------+----------+--------------+--------------+---------------+---------+--------+---------+-----------------+--------+-----------+\n",
      "|        REG|       22821|                  RECCO|                   SAVONA|   SAVONA|2.5238095238095237|                 2.0|              63|2023-01-02|2023-03-31|            13|             4|              1|44.306892|8.470259|    06:28|               63|       0|          1|\n",
      "+-----------+------------+-----------------------+-------------------------+---------+------------------+--------------------+----------------+----------+----------+--------------+--------------+---------------+---------+--------+---------+-----------------+--------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trains_stat3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to elaborate some general stastistics about a train, and store a dataset which is an index of the train ids, and the statistics of that train.\n",
    "\n",
    "# The aggregated statistics that we want to store are: \n",
    "# 1. Average arrival delay at each destination\n",
    "# 2. Median arrival delay at each destination\n",
    "# 3. % of trains with delay > 3 at each destination\n",
    "# 4. % of trains with delay > 5 at each destination\n",
    "# 5. % of trains with delay > 10 at each destination\n",
    "# 6. Number of days the train ran\n",
    "# 7. Number of stops of the train\n",
    "\n",
    "\n",
    "df_trains_stat4 = df_trains_stat3 \\\n",
    "    .groupBy(\"train_id\", \"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\") \\\n",
    "    .agg(\n",
    "        F.avg(\"avg_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.avg(\"median_arrival_delay\").alias(\"median_arrival_delay\"),\n",
    "        (F.avg(\"count_3m_delay\") / F.first(\"count_dates_train\")).alias(\"perc_3m_delay\"),\n",
    "        (F.avg(\"count_5m_delay\") / F.first(\"count_dates_train\")).alias(\"perc_5m_delay\"),\n",
    "        (F.avg(\"count_10m_delay\") / F.first(\"count_dates_train\")).alias(\"perc_10m_delay\"),\n",
    "        F.first(\"count_dates_train\").alias(\"count_dates_train\"),\n",
    "        F.countDistinct(\"stop_name\").alias(\"count_stops_train\"),\n",
    "        F.first(\"first_date\").alias(\"first_date\"),\n",
    "        F.first(\"last_date\").alias(\"last_date\"),\n",
    "        F.min(\"stop_time\").alias(\"departure_time\"),\n",
    "        F.max(\"stop_time\").alias(\"arrival_time\"),\n",
    "    ) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trains_stat4_pandas = df_trains_stat4.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>train_class</th>\n",
       "      <th>train_number</th>\n",
       "      <th>train_departure_stop_name</th>\n",
       "      <th>train_arrival_stop_name</th>\n",
       "      <th>avg_arrival_delay</th>\n",
       "      <th>median_arrival_delay</th>\n",
       "      <th>perc_3m_delay</th>\n",
       "      <th>perc_5m_delay</th>\n",
       "      <th>perc_10m_delay</th>\n",
       "      <th>count_dates_train</th>\n",
       "      <th>count_stops_train</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>arrival_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34359738418</td>\n",
       "      <td>FR</td>\n",
       "      <td>9328</td>\n",
       "      <td>ROMA TERMINI</td>\n",
       "      <td>MANTOVA</td>\n",
       "      <td>2.597628</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.365646</td>\n",
       "      <td>0.221088</td>\n",
       "      <td>0.086735</td>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>19:25</td>\n",
       "      <td>23:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94489280513</td>\n",
       "      <td>REG</td>\n",
       "      <td>4096</td>\n",
       "      <td>ROMA TERMINI</td>\n",
       "      <td>FIRENZE S.M.N.</td>\n",
       "      <td>4.621205</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.281656</td>\n",
       "      <td>0.134740</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>07:02</td>\n",
       "      <td>10:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137438953510</td>\n",
       "      <td>REG</td>\n",
       "      <td>2116</td>\n",
       "      <td>GENOVA P.PRINCIPE</td>\n",
       "      <td>TORINO P.NUOVA</td>\n",
       "      <td>1.689368</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-26</td>\n",
       "      <td>06:27</td>\n",
       "      <td>08:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154618822669</td>\n",
       "      <td>REG</td>\n",
       "      <td>10676</td>\n",
       "      <td>PAVIA</td>\n",
       "      <td>VERCELLI</td>\n",
       "      <td>3.627299</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.219697</td>\n",
       "      <td>0.032197</td>\n",
       "      <td>88</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>12:38</td>\n",
       "      <td>14:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188978561024</td>\n",
       "      <td>REG</td>\n",
       "      <td>827</td>\n",
       "      <td>M N CADORNA</td>\n",
       "      <td>SARONNO</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>09:47</td>\n",
       "      <td>10:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>206158430238</td>\n",
       "      <td>REG</td>\n",
       "      <td>3211</td>\n",
       "      <td>TORINO P.NUOVA</td>\n",
       "      <td>CUNEO</td>\n",
       "      <td>4.411054</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.567416</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.084270</td>\n",
       "      <td>89</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>05:25</td>\n",
       "      <td>06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>223338299438</td>\n",
       "      <td>REG</td>\n",
       "      <td>23209</td>\n",
       "      <td>MONTEBELLUNA</td>\n",
       "      <td>PADOVA</td>\n",
       "      <td>1.715833</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>09:46</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>274877906981</td>\n",
       "      <td>REG</td>\n",
       "      <td>3874</td>\n",
       "      <td>TRIESTE CENTRALE</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.190698</td>\n",
       "      <td>0.096124</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>86</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>06:58</td>\n",
       "      <td>09:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>292057776134</td>\n",
       "      <td>REG</td>\n",
       "      <td>4640</td>\n",
       "      <td>ROMA TERMINI</td>\n",
       "      <td>FIUMICINO AEROPORTO</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.073864</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>15:35</td>\n",
       "      <td>16:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>386547056691</td>\n",
       "      <td>REG</td>\n",
       "      <td>20214</td>\n",
       "      <td>COLLEFERRO-SEGNI-PALIANO</td>\n",
       "      <td>ROMA TERMINI</td>\n",
       "      <td>0.820789</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>19:15</td>\n",
       "      <td>20:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_id train_class train_number train_departure_stop_name  \\\n",
       "0   34359738418          FR         9328              ROMA TERMINI   \n",
       "1   94489280513         REG         4096              ROMA TERMINI   \n",
       "2  137438953510         REG         2116         GENOVA P.PRINCIPE   \n",
       "3  154618822669         REG        10676                     PAVIA   \n",
       "4  188978561024         REG          827               M N CADORNA   \n",
       "5  206158430238         REG         3211            TORINO P.NUOVA   \n",
       "6  223338299438         REG        23209              MONTEBELLUNA   \n",
       "7  274877906981         REG         3874          TRIESTE CENTRALE   \n",
       "8  292057776134         REG         4640              ROMA TERMINI   \n",
       "9  386547056691         REG        20214  COLLEFERRO-SEGNI-PALIANO   \n",
       "\n",
       "  train_arrival_stop_name  avg_arrival_delay  median_arrival_delay  \\\n",
       "0                 MANTOVA           2.597628              0.571429   \n",
       "1          FIRENZE S.M.N.           4.621205              1.071429   \n",
       "2          TORINO P.NUOVA           1.689368             -0.875000   \n",
       "3                VERCELLI           3.627299              2.500000   \n",
       "4                 SARONNO           0.000000              0.000000   \n",
       "5                   CUNEO           4.411054              2.625000   \n",
       "6                  PADOVA           1.715833              1.250000   \n",
       "7     VENEZIA SANTA LUCIA           0.700000             -0.333333   \n",
       "8     FIUMICINO AEROPORTO           0.039773             -0.500000   \n",
       "9            ROMA TERMINI           0.820789              0.555556   \n",
       "\n",
       "   perc_3m_delay  perc_5m_delay  perc_10m_delay  count_dates_train  \\\n",
       "0       0.365646       0.221088        0.086735                 84   \n",
       "1       0.441558       0.281656        0.134740                 88   \n",
       "2       0.166667       0.104167        0.058333                 30   \n",
       "3       0.593750       0.219697        0.032197                 88   \n",
       "4       0.000000       0.000000        0.000000                 88   \n",
       "5       0.567416       0.303371        0.084270                 89   \n",
       "6       0.390000       0.130000        0.000000                 25   \n",
       "7       0.190698       0.096124        0.024031                 86   \n",
       "8       0.073864       0.022727        0.011364                 88   \n",
       "9       0.118280       0.008961        0.000000                 62   \n",
       "\n",
       "   count_stops_train  first_date   last_date departure_time arrival_time  \n",
       "0                  7  2023-01-02  2023-03-31          19:25        23:03  \n",
       "1                 14  2023-01-02  2023-03-31          07:02        10:48  \n",
       "2                  8  2023-01-01  2023-03-26          06:27        08:30  \n",
       "3                 12  2023-01-01  2023-03-31          12:38        14:04  \n",
       "4                  5  2023-01-01  2023-03-31          09:47        10:07  \n",
       "5                  8  2023-01-01  2023-03-31          05:25        06:36  \n",
       "6                  4  2023-02-27  2023-03-31          09:46        10:33  \n",
       "7                 15  2023-01-01  2023-03-31          06:58        09:59  \n",
       "8                  2  2023-01-01  2023-03-31          15:35        16:07  \n",
       "9                  9  2023-01-02  2023-03-31          19:15        20:13  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trains_stat4_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: dataset_generated/data_train_index.csv (deflated 69%)\n"
     ]
    }
   ],
   "source": [
    "if SAVE_COMPUTATIONS :    \n",
    "    # store the file in a csv file  \n",
    "    df_trains_stat4_pandas.to_csv(\"dataset_generated/data_train_index.csv\", index=False)\n",
    "\n",
    "    # zip the file\n",
    "    !zip dataset_generated/data_train_index.zip dataset_generated/data_train_index.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we store the data in a better format, which is a folder in which each file is a train, and the filename is the train id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# if the folder already exists, delete it\n",
    "if os.path.exists(\"renamed_csv_files\"):\n",
    "    shutil.rmtree(\"renamed_csv_files\")\n",
    "\n",
    "# create a new directory to store the renamed CSV files\n",
    "if not os.path.exists(\"renamed_csv_files\"):\n",
    "    os.mkdir(\"renamed_csv_files\")\n",
    "\n",
    "# loop through all directories in the \"data_train_stat/data_train_stat.csv\" directory that start with \"train_id=\"\n",
    "for dirpath, dirnames, filenames in os.walk(\"dataset_generated/data_train_stat/data_train_stat.csv\"):\n",
    "    for dirname in dirnames:\n",
    "        if dirname.startswith(\"train_id=\"):\n",
    "            # extract the ID from the directory name\n",
    "            id = dirname.split(\"=\")[1]\n",
    "            # loop through all CSV files in the directory\n",
    "            for filename in os.listdir(os.path.join(dirpath, dirname)):\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    # rename the file to \"[ID].csv\" and move it to the \"renamed_csv_files\" directory\n",
    "                    src_path = os.path.join(dirpath, dirname, filename)\n",
    "                    dst_path = os.path.join(\"renamed_csv_files\", f\"{id}.csv\")\n",
    "                    shutil.copy(src_path, dst_path)\n",
    "\n",
    "\n",
    "# create a zip file containing the \"renamed_csv_files\" directory, the files have to be in a directory when unzipped\n",
    "shutil.make_archive(\"renamed_csv_files\", \"zip\", \"renamed_csv_files\")\n",
    "\n",
    "# delete the \"renamed_csv_files\" directory\n",
    "shutil.rmtree(\"renamed_csv_files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Path of trains over a map\n",
    "Now that we have computed a dataset with the statistics for each train, in which we managed to extract the timetable, we can use it to plot the path of each train on a map. \n",
    "\n",
    "The task of matching information of a train, with its exact journey on the railway is a complex task. The algorithm developed by [Bast and Brosi (2019)](https://ad-publications.cs.uni-freiburg.de/SIGSPATIAL_Sparse%20map%20matching%202018.pdf) matches the GTFS schedule of a train with the OpenStreetMap railway network. Thankfully, they published the code of this algorithm on GitHub. Their library takes a GTFS schedule of a train and a railway network in OpenStreetMap format and returns the most likely journey of the train on the railway network in `shapefile` format. GTFS is a standard format for public transport schedules, promoted by Google.\n",
    "\n",
    "First, we need to generate a GTFS timetable for our trains. Then, we can run the algorithm a produce a shapefile for each train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trains_timetable = df_trains_stat3 \\\n",
    "    .select(\"train_id\", \"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\", \"stop_number\", \"stop_name\", \"stop_time\", \"stop_lat\", \"stop_lon\")\n",
    "\n",
    "# need to join with the stops to get stop_id that we have lost on the way\n",
    "trains_timetable = trains_timetable \\\n",
    "    .join(stops, on=[\"stop_name\", \"stop_lat\", \"stop_lon\"], how=\"inner\")\n",
    "\n",
    "trains_timetable_pandas = trains_timetable.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>train_id</th>\n",
       "      <th>train_class</th>\n",
       "      <th>train_number</th>\n",
       "      <th>train_departure_stop_name</th>\n",
       "      <th>train_arrival_stop_name</th>\n",
       "      <th>stop_number</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_name_short</th>\n",
       "      <th>stop_id_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAVONA</td>\n",
       "      <td>44.306892</td>\n",
       "      <td>8.470259</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>1</td>\n",
       "      <td>06:28</td>\n",
       "      <td>S04801</td>\n",
       "      <td>Savona</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENOVA VOLTRI</td>\n",
       "      <td>44.428467</td>\n",
       "      <td>8.758163</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>2</td>\n",
       "      <td>06:48</td>\n",
       "      <td>S04534</td>\n",
       "      <td>Genova Voltri</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENOVA SESTRI PONENTE</td>\n",
       "      <td>44.422374</td>\n",
       "      <td>8.847707</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>3</td>\n",
       "      <td>07:02</td>\n",
       "      <td>S04537</td>\n",
       "      <td>Genova Sestri P.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GENOVA SAMPIERDARENA</td>\n",
       "      <td>44.413102</td>\n",
       "      <td>8.887271</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>4</td>\n",
       "      <td>07:13</td>\n",
       "      <td>S04220</td>\n",
       "      <td>GE Sampierdarena</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GENOVA P.PRINCIPE</td>\n",
       "      <td>44.417784</td>\n",
       "      <td>8.920700</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>5</td>\n",
       "      <td>07:20</td>\n",
       "      <td>S04700</td>\n",
       "      <td>Genova P.Princ.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENOVA BRIGNOLE</td>\n",
       "      <td>44.407213</td>\n",
       "      <td>8.947553</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>6</td>\n",
       "      <td>07:31</td>\n",
       "      <td>S04702</td>\n",
       "      <td>Genova Brignole</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GENOVA QUARTO DEI MILLE</td>\n",
       "      <td>44.388607</td>\n",
       "      <td>8.995970</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>7</td>\n",
       "      <td>07:41</td>\n",
       "      <td>S04704</td>\n",
       "      <td>Genova Quarto</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GENOVA NERVI</td>\n",
       "      <td>44.381271</td>\n",
       "      <td>9.039895</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>8</td>\n",
       "      <td>07:48</td>\n",
       "      <td>S04707</td>\n",
       "      <td>Genova Nervi</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RECCO</td>\n",
       "      <td>44.361215</td>\n",
       "      <td>9.146788</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>9</td>\n",
       "      <td>08:08</td>\n",
       "      <td>S04714</td>\n",
       "      <td>Recco</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SARONNO</td>\n",
       "      <td>45.625309</td>\n",
       "      <td>9.030839</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>876</td>\n",
       "      <td>SARONNO</td>\n",
       "      <td>M N CADORNA</td>\n",
       "      <td>1</td>\n",
       "      <td>19:53</td>\n",
       "      <td>S01933</td>\n",
       "      <td>SARONNO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 stop_name   stop_lat  stop_lon  train_id train_class  \\\n",
       "0                   SAVONA  44.306892  8.470259         0         REG   \n",
       "1            GENOVA VOLTRI  44.428467  8.758163         0         REG   \n",
       "2    GENOVA SESTRI PONENTE  44.422374  8.847707         0         REG   \n",
       "3     GENOVA SAMPIERDARENA  44.413102  8.887271         0         REG   \n",
       "4        GENOVA P.PRINCIPE  44.417784  8.920700         0         REG   \n",
       "5          GENOVA BRIGNOLE  44.407213  8.947553         0         REG   \n",
       "6  GENOVA QUARTO DEI MILLE  44.388607  8.995970         0         REG   \n",
       "7             GENOVA NERVI  44.381271  9.039895         0         REG   \n",
       "8                    RECCO  44.361215  9.146788         0         REG   \n",
       "9                  SARONNO  45.625309  9.030839         1         REG   \n",
       "\n",
       "  train_number train_departure_stop_name train_arrival_stop_name  stop_number  \\\n",
       "0        22821                    SAVONA                   RECCO            1   \n",
       "1        22821                    SAVONA                   RECCO            2   \n",
       "2        22821                    SAVONA                   RECCO            3   \n",
       "3        22821                    SAVONA                   RECCO            4   \n",
       "4        22821                    SAVONA                   RECCO            5   \n",
       "5        22821                    SAVONA                   RECCO            6   \n",
       "6        22821                    SAVONA                   RECCO            7   \n",
       "7        22821                    SAVONA                   RECCO            8   \n",
       "8        22821                    SAVONA                   RECCO            9   \n",
       "9          876                   SARONNO             M N CADORNA            1   \n",
       "\n",
       "  stop_time stop_id   stop_name_short  stop_id_region  \n",
       "0     06:28  S04801            Savona             2.0  \n",
       "1     06:48  S04534     Genova Voltri             2.0  \n",
       "2     07:02  S04537  Genova Sestri P.             2.0  \n",
       "3     07:13  S04220  GE Sampierdarena             2.0  \n",
       "4     07:20  S04700   Genova P.Princ.             2.0  \n",
       "5     07:31  S04702   Genova Brignole             2.0  \n",
       "6     07:41  S04704     Genova Quarto             2.0  \n",
       "7     07:48  S04707      Genova Nervi             2.0  \n",
       "8     08:08  S04714             Recco             2.0  \n",
       "9     19:53  S01933           SARONNO             1.0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains_timetable_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REG', 'IC', 'FR', 'EC', 'FA', 'FB', 'ICN'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at distinct values of train_class\n",
    "trains_timetable_pandas[\"train_class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# delete folder if it exists\n",
    "import shutil\n",
    "if os.path.exists(\"gtfs\"):\n",
    "    shutil.rmtree(\"gtfs\")\n",
    "\n",
    "# create folder gtfs \n",
    "if not os.path.exists(\"gtfs\"):\n",
    "    os.mkdir(\"gtfs\")\n",
    "\n",
    "# 1. Create the agency.txt file\n",
    "agency = {\n",
    "    \"agency_id\": \"1\",\n",
    "    \"agency_name\": \"Trenitalia\",\n",
    "    \"agency_url\": \"https://www.trenitalia.com\",\n",
    "    \"agency_timezone\": \"Europe/Rome\",\n",
    "    \"agency_lang\": \"it\",\n",
    "    \"agency_phone\": \"\"\n",
    "}\n",
    "agency = pd.DataFrame(agency, index=[0])\n",
    "agency.to_csv(\"gtfs/agency.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create routes.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#routestxt\n",
    "\n",
    "\n",
    "routes = pd.DataFrame()\n",
    "\n",
    "# get unique tuples (train_id, train_class, train_number)\n",
    "train_ids = trains_timetable_pandas[[\"train_id\", \"train_class\", \"train_number\"]].drop_duplicates()\n",
    "\n",
    "routes[\"route_id\"] = train_ids[\"train_id\"]\n",
    "routes[\"route_short_name\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "routes[\"route_long_name\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "\n",
    "color_mapper = {\n",
    "    \"IC\": \"E0A434\",\n",
    "    \"REG\": \"036864\",\n",
    "    \"ICN\": \"E0A434\",\n",
    "    \"AV\": \"DC263B\",\n",
    "    \"EC\": \"DC263B\",\n",
    "    \"FR\": \"DC263B\",\n",
    "    \"FA\": \"DC263B\",\n",
    "    \"FB\": \"DC263B\",\n",
    "}\n",
    "\n",
    "routes[\"agency_id\"] = \"1\"\n",
    "routes[\"route_type\"] = \"2\" # 2 is train\n",
    "\n",
    "routes[\"route_color\"] = routes[\"route_short_name\"].apply(lambda x: color_mapper[x.split(\" \")[0]])\n",
    "\n",
    "routes.to_csv(\"gtfs/routes.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create stops.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#stopstxt\n",
    "stops_gtfs = trains_timetable_pandas[[\"stop_id\", \"stop_name\", \"stop_lat\", \"stop_lon\"]].drop_duplicates()\n",
    "stops_gtfs.to_csv(\"gtfs/stops.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tough one, create calendar.txt and trips.txt\n",
    "# calendar describes the span of a service\n",
    "# trips describes the service for a particular route\n",
    "\n",
    "# 4a: create calendar.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#calendartxt\n",
    "calendar = {\n",
    "    # we just create a service that runs every day for the whole year\n",
    "    \"service_id\": \"1\",\n",
    "    \"monday\": \"1\",\n",
    "    \"tuesday\": \"1\",\n",
    "    \"wednesday\": \"1\",\n",
    "    \"thursday\": \"1\",\n",
    "    \"friday\": \"1\",\n",
    "    \"saturday\": \"1\",\n",
    "    \"sunday\": \"1\",\n",
    "    \"start_date\": \"19000101\",\n",
    "    \"end_date\": \"21000101\",\n",
    "}\n",
    "calendar = pd.DataFrame(calendar, index=[0])\n",
    "calendar.to_csv(\"gtfs/calendar.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: create trips.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#tripstxt\n",
    "# trips = {\n",
    "#     \"route_id\": \"1\",\n",
    "#     \"service_id\": \"1\",\n",
    "#     \"trip_id\": \"1\",\n",
    "#     \"trip_headsign\": query_7[\"train_class\"] + \" \" + query_7[\"train_number\"],\n",
    "#     \"trip_short_name\": query_7[\"train_class\"] + \" \" + query_7[\"train_number\"],\n",
    "#     \"direction_id\": \"\",\n",
    "#     \"block_id\": \"\",\n",
    "#     \"shape_id\": \"\",\n",
    "#     \"wheelchair_accessible\": \"\",\n",
    "#     \"bikes_allowed\": \"\",\n",
    "# }\n",
    "\n",
    "# trips = pd.DataFrame(trips, index=[0])\n",
    "\n",
    "trips = pd.DataFrame()\n",
    "\n",
    "trips[\"route_id\"] = train_ids[\"train_id\"]\n",
    "trips[\"service_id\"] = \"1\"\n",
    "trips[\"trip_id\"] = train_ids[\"train_id\"]\n",
    "trips[\"trip_headsign\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "trips[\"trip_short_name\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "trips[\"direction_id\"] = \"\"\n",
    "trips[\"block_id\"] = \"\"\n",
    "trips[\"shape_id\"] = \"\"\n",
    "trips[\"wheelchair_accessible\"] = \"\"\n",
    "trips[\"bikes_allowed\"] = \"\"\n",
    "\n",
    "\n",
    "trips.to_csv(\"gtfs/trips.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>train_id</th>\n",
       "      <th>train_class</th>\n",
       "      <th>train_number</th>\n",
       "      <th>train_departure_stop_name</th>\n",
       "      <th>train_arrival_stop_name</th>\n",
       "      <th>stop_number</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_name_short</th>\n",
       "      <th>stop_id_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAVONA</td>\n",
       "      <td>44.306892</td>\n",
       "      <td>8.470259</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>1</td>\n",
       "      <td>06:28</td>\n",
       "      <td>S04801</td>\n",
       "      <td>Savona</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENOVA VOLTRI</td>\n",
       "      <td>44.428467</td>\n",
       "      <td>8.758163</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>2</td>\n",
       "      <td>06:48</td>\n",
       "      <td>S04534</td>\n",
       "      <td>Genova Voltri</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENOVA SESTRI PONENTE</td>\n",
       "      <td>44.422374</td>\n",
       "      <td>8.847707</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>3</td>\n",
       "      <td>07:02</td>\n",
       "      <td>S04537</td>\n",
       "      <td>Genova Sestri P.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GENOVA SAMPIERDARENA</td>\n",
       "      <td>44.413102</td>\n",
       "      <td>8.887271</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>4</td>\n",
       "      <td>07:13</td>\n",
       "      <td>S04220</td>\n",
       "      <td>GE Sampierdarena</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GENOVA P.PRINCIPE</td>\n",
       "      <td>44.417784</td>\n",
       "      <td>8.920700</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>5</td>\n",
       "      <td>07:20</td>\n",
       "      <td>S04700</td>\n",
       "      <td>Genova P.Princ.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENOVA BRIGNOLE</td>\n",
       "      <td>44.407213</td>\n",
       "      <td>8.947553</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>6</td>\n",
       "      <td>07:31</td>\n",
       "      <td>S04702</td>\n",
       "      <td>Genova Brignole</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GENOVA QUARTO DEI MILLE</td>\n",
       "      <td>44.388607</td>\n",
       "      <td>8.995970</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>7</td>\n",
       "      <td>07:41</td>\n",
       "      <td>S04704</td>\n",
       "      <td>Genova Quarto</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GENOVA NERVI</td>\n",
       "      <td>44.381271</td>\n",
       "      <td>9.039895</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>8</td>\n",
       "      <td>07:48</td>\n",
       "      <td>S04707</td>\n",
       "      <td>Genova Nervi</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RECCO</td>\n",
       "      <td>44.361215</td>\n",
       "      <td>9.146788</td>\n",
       "      <td>0</td>\n",
       "      <td>REG</td>\n",
       "      <td>22821</td>\n",
       "      <td>SAVONA</td>\n",
       "      <td>RECCO</td>\n",
       "      <td>9</td>\n",
       "      <td>08:08</td>\n",
       "      <td>S04714</td>\n",
       "      <td>Recco</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SARONNO</td>\n",
       "      <td>45.625309</td>\n",
       "      <td>9.030839</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>876</td>\n",
       "      <td>SARONNO</td>\n",
       "      <td>M N CADORNA</td>\n",
       "      <td>1</td>\n",
       "      <td>19:53</td>\n",
       "      <td>S01933</td>\n",
       "      <td>SARONNO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 stop_name   stop_lat  stop_lon  train_id train_class  \\\n",
       "0                   SAVONA  44.306892  8.470259         0         REG   \n",
       "1            GENOVA VOLTRI  44.428467  8.758163         0         REG   \n",
       "2    GENOVA SESTRI PONENTE  44.422374  8.847707         0         REG   \n",
       "3     GENOVA SAMPIERDARENA  44.413102  8.887271         0         REG   \n",
       "4        GENOVA P.PRINCIPE  44.417784  8.920700         0         REG   \n",
       "5          GENOVA BRIGNOLE  44.407213  8.947553         0         REG   \n",
       "6  GENOVA QUARTO DEI MILLE  44.388607  8.995970         0         REG   \n",
       "7             GENOVA NERVI  44.381271  9.039895         0         REG   \n",
       "8                    RECCO  44.361215  9.146788         0         REG   \n",
       "9                  SARONNO  45.625309  9.030839         1         REG   \n",
       "\n",
       "  train_number train_departure_stop_name train_arrival_stop_name  stop_number  \\\n",
       "0        22821                    SAVONA                   RECCO            1   \n",
       "1        22821                    SAVONA                   RECCO            2   \n",
       "2        22821                    SAVONA                   RECCO            3   \n",
       "3        22821                    SAVONA                   RECCO            4   \n",
       "4        22821                    SAVONA                   RECCO            5   \n",
       "5        22821                    SAVONA                   RECCO            6   \n",
       "6        22821                    SAVONA                   RECCO            7   \n",
       "7        22821                    SAVONA                   RECCO            8   \n",
       "8        22821                    SAVONA                   RECCO            9   \n",
       "9          876                   SARONNO             M N CADORNA            1   \n",
       "\n",
       "  stop_time stop_id   stop_name_short  stop_id_region  \n",
       "0     06:28  S04801            Savona             2.0  \n",
       "1     06:48  S04534     Genova Voltri             2.0  \n",
       "2     07:02  S04537  Genova Sestri P.             2.0  \n",
       "3     07:13  S04220  GE Sampierdarena             2.0  \n",
       "4     07:20  S04700   Genova P.Princ.             2.0  \n",
       "5     07:31  S04702   Genova Brignole             2.0  \n",
       "6     07:41  S04704     Genova Quarto             2.0  \n",
       "7     07:48  S04707      Genova Nervi             2.0  \n",
       "8     08:08  S04714             Recco             2.0  \n",
       "9     19:53  S01933           SARONNO             1.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains_timetable_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create stop_times.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#stop_timestxt\n",
    "# convert stop_departure_time to HH:MM:SS using pyspark function\n",
    "\n",
    "# stop_times: pd.DataFrame = train_data_day_df[[\"stop_id\", \"stop_departure_time\", \"stop_arrival_time\"]] \\\n",
    "#     .rename(columns={\"stop_id\": \"stop_id\", \"stop_departure_time\": \"departure_time\", \"stop_arrival_time\": \"arrival_time\"})\n",
    "\n",
    "\n",
    "# def convert_to_hh_mm_ss(time):\n",
    "    \n",
    "                                                                                                            \n",
    "# # add column with stop_sequence\n",
    "# stop_times[\"stop_sequence\"] = stop_times.index + 1\n",
    "# # add column with trip_id\n",
    "# stop_times[\"trip_id\"] = \"1\"\n",
    "# # put departure time of the last stop equal to the arrival time\n",
    "# stop_times.loc[stop_times.index[-1], \"departure_time\"] = stop_times.loc[stop_times.index[-1], \"arrival_time\"]\n",
    "# store it\n",
    "\n",
    "stop_times_gtfs = pd.DataFrame()\n",
    "\n",
    "stop_times_gtfs[\"trip_id\"] = trains_timetable_pandas[\"train_id\"]\n",
    "stop_times_gtfs[\"arrival_time\"] = trains_timetable_pandas[\"stop_time\"].apply(lambda x: x + \":00\")\n",
    "stop_times_gtfs[\"departure_time\"] = trains_timetable_pandas[\"stop_time\"].apply(lambda x: x + \":00\")\n",
    "stop_times_gtfs[\"stop_id\"] = trains_timetable_pandas[\"stop_id\"]\n",
    "stop_times_gtfs[\"stop_sequence\"] = trains_timetable_pandas[\"stop_number\"]\n",
    "\n",
    "stop_times_gtfs.to_csv(\"gtfs/stop_times.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a zip using libzip\n",
    "import zipfile\n",
    "zf = zipfile.ZipFile('gtfs.zip', mode='w')\n",
    "try:\n",
    "    zf.write(\"gtfs/agency.txt\")\n",
    "    zf.write(\"gtfs/calendar.txt\")\n",
    "    zf.write(\"gtfs/routes.txt\")\n",
    "    zf.write(\"gtfs/stops.txt\")\n",
    "    zf.write(\"gtfs/stop_times.txt\")\n",
    "    zf.write(\"gtfs/trips.txt\")\n",
    "finally:\n",
    "    zf.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the algorithm\n",
    "We downloaded and installed pfaedle as well as an OSM dump for Italy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current folder name\n",
    "# import os\n",
    "# os.path.basename(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pfaedle -x ~/Downloads/italy-latest.osm {}\"\"\".format(os.path.join(os.getcwd(), \"gtfs\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the shapes in the \"shapes.txt\" file, we want to separate all those files into one file for each train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_id</th>\n",
       "      <th>shape_pt_lat</th>\n",
       "      <th>shape_pt_lon</th>\n",
       "      <th>shape_pt_sequence</th>\n",
       "      <th>shape_dist_traveled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.760090</td>\n",
       "      <td>10.996771</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.760235</td>\n",
       "      <td>10.996886</td>\n",
       "      <td>2</td>\n",
       "      <td>18.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.761852</td>\n",
       "      <td>10.998228</td>\n",
       "      <td>3</td>\n",
       "      <td>226.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.762783</td>\n",
       "      <td>10.998978</td>\n",
       "      <td>4</td>\n",
       "      <td>345.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.763184</td>\n",
       "      <td>10.999331</td>\n",
       "      <td>5</td>\n",
       "      <td>397.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.763657</td>\n",
       "      <td>10.999795</td>\n",
       "      <td>6</td>\n",
       "      <td>461.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.764145</td>\n",
       "      <td>11.000370</td>\n",
       "      <td>7</td>\n",
       "      <td>531.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.764538</td>\n",
       "      <td>11.000874</td>\n",
       "      <td>8</td>\n",
       "      <td>590.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.764877</td>\n",
       "      <td>11.001401</td>\n",
       "      <td>9</td>\n",
       "      <td>646.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.765221</td>\n",
       "      <td>11.001960</td>\n",
       "      <td>10</td>\n",
       "      <td>704.023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  shape_dist_traveled\n",
       "0  shp_2_1     45.760090     10.996771                  1                0.000\n",
       "1  shp_2_1     45.760235     10.996886                  2               18.672\n",
       "2  shp_2_1     45.761852     10.998228                  3              226.667\n",
       "3  shp_2_1     45.762783     10.998978                  4              345.598\n",
       "4  shp_2_1     45.763184     10.999331                  5              397.773\n",
       "5  shp_2_1     45.763657     10.999795                  6              461.719\n",
       "6  shp_2_1     45.764145     11.000370                  7              531.932\n",
       "7  shp_2_1     45.764538     11.000874                  8              590.560\n",
       "8  shp_2_1     45.764877     11.001401                  9              646.452\n",
       "9  shp_2_1     45.765221     11.001960                 10              704.023"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shapes_gtfs = pd.read_csv(\"gtfs-out/shapes.txt\")\n",
    "trips_gtfs = pd.read_csv(\"gtfs-out/trips.txt\")\n",
    "\n",
    "shapes_gtfs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_gtfs = trips_gtfs[[\"trip_id\",\"shape_id\"]]\n",
    "# merge columns on shape_id\n",
    "shapes_gtfs_merged = shapes_gtfs.merge(trips_gtfs, on=\"shape_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct trains is  9999 while the number of distinct shapes is  2721\n"
     ]
    }
   ],
   "source": [
    "# drop shape_id\n",
    "print(\"The number of distinct trains is \", shapes_gtfs_merged[\"trip_id\"].nunique(), \"while the number of distinct shapes is \", shapes_gtfs_merged[\"shape_id\"].nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could save some space by storing the shapes independently from the trains, since there's multiple trains that share the same shape. However, to keep the frontend of the website simple, we decided to store the shape of each train in a separated file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1529008357401, 1546188226587,  188978561055, ...,  781684047876,\n",
       "        850403524645, 1142461300786])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(shapes_gtfs_merged[\"trip_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:46<00:00, 216.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# delete folder if exists\n",
    "import shutil\n",
    "if os.path.exists(\"dataset_generated/trains_shapes\"):\n",
    "    shutil.rmtree(\"dataset_generated/trains_shapes\")\n",
    "\n",
    "# create folder\n",
    "os.mkdir(\"dataset_generated/trains_shapes\")\n",
    "\n",
    "# for each trid_id, save all the corrisponding shapes in a file\n",
    "for trip_id in tqdm(pd.unique(shapes_gtfs_merged[\"trip_id\"])):\n",
    "    shapes_gtfs_merged[shapes_gtfs_merged[\"trip_id\"] == trip_id][[\"shape_pt_lat\", \"shape_pt_lon\"]].to_csv(\"dataset_generated/trains_shapes/{}.csv\".format(trip_id), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the folder with shutil\n",
    "shutil.make_archive(\"dataset_generated/trains_shapes\", 'zip', \"dataset_generated/trains_shapes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrenitaliaSpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
