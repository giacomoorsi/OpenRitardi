{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to generate some of the files used by our project website. \n",
    "\n",
    "Data is taken from [OpenStats](https://openstats.altervista.org) and has to be pre-processed in the [Data Wrangling Notebook](data_wrangling.ipynb). This notebook assumes you have already pre-preprocessed the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Initial Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import lit, col, to_date\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import stddev\n",
    "from pyspark.sql.functions import count, countDistinct, concat, sum\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"...\" # replace with the output folder of the data_wrangling script\n",
    "file = \"all.parquet\"\n",
    "\n",
    "SAVE_COMPUTATIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/23 22:25:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Trenitalia\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '64729'),\n",
       " ('spark.driver.host', '10.5.53.40'),\n",
       " ('spark.app.id', 'local-1706045130609'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.app.name', 'Trenitalia'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.startTime', '1706045128926'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.submitTime', '1706045128543'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================>                        (8 + 6) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 32029349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(os.path.join(DATA_FOLDER, \"parquet\", file))\n",
    "print(\"Number of rows: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_arrival_stop_name',\n",
       " 'train_class',\n",
       " 'train_cn',\n",
       " 'train_dl',\n",
       " 'train_number',\n",
       " 'train_arrival_time',\n",
       " 'oae',\n",
       " 'train_oaz',\n",
       " 'train_od',\n",
       " 'train_oo',\n",
       " 'train_departure_time',\n",
       " 'train_ope',\n",
       " 'train_opz',\n",
       " 'train_departure_stop_name',\n",
       " 'train_pr',\n",
       " 'train_arrival_delay',\n",
       " 'train_departure_delay',\n",
       " 'sea',\n",
       " 'train_sep',\n",
       " 'train_sub',\n",
       " 'day',\n",
       " 'month',\n",
       " 'year',\n",
       " 'date',\n",
       " '_id',\n",
       " 'stop_name',\n",
       " 'stop_arrival_time',\n",
       " 'stop_departure_time',\n",
       " 'stop_arrival_delay',\n",
       " 'stop_departure_delay']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(100, \"stop_name\", \"train_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset of the stops\n",
    "stops = spark.read.csv(\"stops.csv\", header=True, inferSchema=True)\n",
    "\n",
    "stops_column_renamer = {\n",
    "    \"name\": \"stop_name\",\n",
    "    \"lat\": \"stop_lat\",\n",
    "    \"lon\": \"stop_lon\",\n",
    "    \"station_id\": \"stop_id\", \n",
    "    \"name_short\": \"stop_name_short\",\n",
    "    \"id_region\": \"stop_id_region\",\n",
    "}\n",
    "\n",
    "for k, v in stops_column_renamer.items():\n",
    "    stops = stops.withColumnRenamed(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days:  365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date:  2023-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last date:  2023-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trains:  16011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops:  2426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train classes:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:================================================>      (88 + 8) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dataset rows:  32029349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# number of days\n",
    "print(\"Number of days: \", df.select(\"date\").distinct().count())\n",
    "\n",
    "# first date\n",
    "print(\"First date: \", df.select(\"date\").distinct().orderBy(\"date\").first().asDict()[\"date\"])\n",
    "\n",
    "# last date\n",
    "print(\"Last date: \", df.select(\"date\").distinct().orderBy(\"date\", ascending=False).first().asDict()[\"date\"])\n",
    "\n",
    "# number of trains\n",
    "print(\"Number of trains: \", df.select(\"train_number\").distinct().count())\n",
    "\n",
    "# number of stops\n",
    "print(\"Number of stops: \", df.select(\"stop_name\").distinct().count())\n",
    "\n",
    "# number of train classes\n",
    "print(\"Number of train classes: \", df.select(\"train_class\").distinct().count())\n",
    "\n",
    "# number of dataset rows\n",
    "print(\"Number of dataset rows: \", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_arrival_stop_name', 'train_class', 'train_cn', 'train_dl', 'train_number', 'train_arrival_time', 'oae', 'train_oaz', 'train_od', 'train_oo', 'train_departure_time', 'train_ope', 'train_opz', 'train_departure_stop_name', 'train_pr', 'train_arrival_delay', 'train_departure_delay', 'sea', 'train_sep', 'train_sub', 'day', 'month', 'year', 'date', '_id', 'stop_name', 'stop_arrival_time', 'stop_departure_time', 'stop_arrival_delay', 'stop_departure_delay']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preprocessing\n",
    "As step of preprocessing, we remove all delays that are anomalous, i.e. they are not in the range [-100, 300] minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column oae: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_oaz: 38247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_od: 391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_oo: 397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_cn: 976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_dl: 4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_ope: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_opz: 35793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_pr: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column sea: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_sep: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 176:===========================================>          (80 + 8) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for column train_sub: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "interesting_columns = ['oae', 'train_oaz', \"train_od\", \"train_oo\", \"train_cn\", \"train_dl\", \"train_ope\", \"train_opz\", \"train_pr\", \"sea\", \"train_sep\", \"train_sub\" ]\n",
    "\n",
    "# for each of the interesting_columns compute and show the distinct values\n",
    "for c in interesting_columns:\n",
    "    print(\"Distinct values for column {}: {}\".format(c, df.select(c).distinct().count()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all values of delays that are not in the range [-100, 300] if they are numerical\n",
    "MIN_DELAY = -100\n",
    "MAX_DELAY = 300\n",
    "df = df.filter((col(\"stop_arrival_delay\").cast(\"double\").isNull()) | (col(\"stop_arrival_delay\").cast(\"double\") >= MIN_DELAY) & (col(\"stop_arrival_delay\").cast(\"double\") <= MAX_DELAY))\n",
    "\n",
    "# for trains that have a non-null `train_sub`, we replace `train_class` with `train_sub`\n",
    "# NOTE: `train_sub` contains the category of high speed train (e.g. \"FR\", \"FB\", \"FA\" which stand for Frecciarossa, Frecciabianca, Frecciargento)\n",
    "# When a train is high speed, `train_class` is empty, apart from one case, which is the FrecciaRossa Milano -> Paris where the train_class is EC (EuroCity)\n",
    "# We replace that with FR (FrecciaRossa)\n",
    "df = df.withColumn(\"train_class\", F.when(col(\"train_sub\").isNotNull(), col(\"train_sub\")).otherwise(col(\"train_class\")))\n",
    "\n",
    "# remove all the trains that are not in the following classes\n",
    "KEEP_TRAIN_CLASSES = [\"IC\", \"ICN\", \"REG\", \"\", \"EC\", \"FA\", \"FB\", \"FR\"]\n",
    "\n",
    "df = df.filter(col(\"train_class\").isin(KEEP_TRAIN_CLASSES))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Statistics for each station"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each distinct station, we want to obtain: \n",
    "1. Station name\n",
    "2. Latitude, longitude\n",
    "3. Average arrival delay\n",
    "4. Median arrival delay\n",
    "5. % of trains with delay > 3\n",
    "6. % of trains with delay > 5\n",
    "7. % of trains with delay > 10\n",
    "8. Number of distinct train numbers that stopped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe that counts the number of trains passed by each stop\n",
    "stop_counts = df.groupBy(\"stop_name\").agg(F.count(\"train_number\").alias(\"count_stops\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column True if train had > 3 minutes of delay\n",
    "data_stop = df.join(stops, on=\"stop_name\", how=\"inner\")\n",
    "\n",
    "data_stop = data_stop \\\n",
    "    .filter(col(\"stop_arrival_delay\").cast(\"double\").isNotNull()) \\\n",
    "    .withColumn(\"stop_arrival_delay_double\", col(\"stop_arrival_delay\").cast(\"double\")) \\\n",
    "    .drop(\"stop_arrival_delay\") \\\n",
    "    .withColumnRenamed(\"stop_arrival_delay_double\", \"stop_arrival_delay\") \\\n",
    "    .withColumn(\"3m_delay\", col(\"stop_arrival_delay\") > 3)\\\n",
    "    .withColumn(\"5m_delay\", col(\"stop_arrival_delay\") > 5)\\\n",
    "    .withColumn(\"10m_delay\", col(\"stop_arrival_delay\") > 10)\\\n",
    "    .withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\"))\\\n",
    "    .withColumn(\"train_id\", concat(col(\"train_class\"), col(\"train_number\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stop_stat = data_stop.groupBy(\"stop_name\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        # F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=\"stop_name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:==========================================>          (161 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops:  2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of stops: \", data_stop_stat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops in (lat,long) dataset:  2961\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stops in (lat,long) dataset: \", stops.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:==================================================>   (94 + 6) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stops in final dataset:  2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of stops in final dataset: \", data_stop_stat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we hardcode a minimum threshold, at least one train per month\n",
    "MIN_THRESHOLD_DATES = 12 # at least for 12 different dates a train must have stopped here\n",
    "MIN_THRESHOLD_STOP = 12 # at least 12 times a train must have stopped here\n",
    "data_stop_stat = data_stop_stat.filter(col(\"count_stops\") >= MIN_THRESHOLD_STOP)\n",
    "\n",
    "# we filter out stops without lat/lon\n",
    "data_stop_stat = data_stop_stat.filter(col(\"stop_lat\").isNotNull() & col(\"stop_lon\").isNotNull())\n",
    "\n",
    "data_stop_stat = data_stop_stat.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>avg_arrival_delay</th>\n",
       "      <th>median_arrival_delay</th>\n",
       "      <th>count_trains</th>\n",
       "      <th>count_3m_delay</th>\n",
       "      <th>count_5m_delay</th>\n",
       "      <th>count_10m_delay</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>count_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZOAGLI</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>44.335460</td>\n",
       "      <td>9.269126</td>\n",
       "      <td>13178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALA SABINA</td>\n",
       "      <td>2.086808</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>210</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>41.011012</td>\n",
       "      <td>9.588100</td>\n",
       "      <td>1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROCCA D`EVANDRO</td>\n",
       "      <td>4.443245</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34</td>\n",
       "      <td>2241</td>\n",
       "      <td>1440</td>\n",
       "      <td>689</td>\n",
       "      <td>41.438861</td>\n",
       "      <td>13.911338</td>\n",
       "      <td>6404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TERONTOLA CORTONA</td>\n",
       "      <td>5.615185</td>\n",
       "      <td>3.0</td>\n",
       "      <td>161</td>\n",
       "      <td>10524</td>\n",
       "      <td>6938</td>\n",
       "      <td>3108</td>\n",
       "      <td>43.210263</td>\n",
       "      <td>12.007906</td>\n",
       "      <td>25167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MONTIRONE</td>\n",
       "      <td>6.039469</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4090</td>\n",
       "      <td>3065</td>\n",
       "      <td>1515</td>\n",
       "      <td>45.451352</td>\n",
       "      <td>10.239482</td>\n",
       "      <td>8330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BORGO FORNARI PER VOLTAGGIO</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>44.588379</td>\n",
       "      <td>8.938295</td>\n",
       "      <td>10027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SAN MARCO D`ALUNZIO T.</td>\n",
       "      <td>3.005983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1675</td>\n",
       "      <td>874</td>\n",
       "      <td>255</td>\n",
       "      <td>38.094202</td>\n",
       "      <td>14.675909</td>\n",
       "      <td>6918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>VEZZANO LIGURE</td>\n",
       "      <td>2.754894</td>\n",
       "      <td>2.0</td>\n",
       "      <td>107</td>\n",
       "      <td>4478</td>\n",
       "      <td>2681</td>\n",
       "      <td>827</td>\n",
       "      <td>44.127081</td>\n",
       "      <td>9.889280</td>\n",
       "      <td>15411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ZINASCO NUOVO</td>\n",
       "      <td>6.222046</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34</td>\n",
       "      <td>5047</td>\n",
       "      <td>2920</td>\n",
       "      <td>790</td>\n",
       "      <td>45.124555</td>\n",
       "      <td>8.999657</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PRAJA AJETA TORTORA</td>\n",
       "      <td>4.129447</td>\n",
       "      <td>2.0</td>\n",
       "      <td>58</td>\n",
       "      <td>3476</td>\n",
       "      <td>2471</td>\n",
       "      <td>1112</td>\n",
       "      <td>39.900832</td>\n",
       "      <td>15.780628</td>\n",
       "      <td>10202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      stop_name  avg_arrival_delay  median_arrival_delay  \\\n",
       "0                        ZOAGLI          11.600000                  10.0   \n",
       "1                   CALA SABINA           2.086808                   2.0   \n",
       "2               ROCCA D`EVANDRO           4.443245                   2.0   \n",
       "3             TERONTOLA CORTONA           5.615185                   3.0   \n",
       "4                     MONTIRONE           6.039469                   3.0   \n",
       "..                          ...                ...                   ...   \n",
       "95  BORGO FORNARI PER VOLTAGGIO          20.200000                  16.0   \n",
       "96       SAN MARCO D`ALUNZIO T.           3.005983                   1.0   \n",
       "97               VEZZANO LIGURE           2.754894                   2.0   \n",
       "98                ZINASCO NUOVO           6.222046                   5.0   \n",
       "99          PRAJA AJETA TORTORA           4.129447                   2.0   \n",
       "\n",
       "    count_trains  count_3m_delay  count_5m_delay  count_10m_delay   stop_lat  \\\n",
       "0             29              31              28               18  44.335460   \n",
       "1             20             210              42               12  41.011012   \n",
       "2             34            2241            1440              689  41.438861   \n",
       "3            161           10524            6938             3108  43.210263   \n",
       "4             30            4090            3065             1515  45.451352   \n",
       "..           ...             ...             ...              ...        ...   \n",
       "95            10               9               9                7  44.588379   \n",
       "96            30            1675             874              255  38.094202   \n",
       "97           107            4478            2681              827  44.127081   \n",
       "98            34            5047            2920              790  45.124555   \n",
       "99            58            3476            2471             1112  39.900832   \n",
       "\n",
       "     stop_lon  count_stops  \n",
       "0    9.269126        13178  \n",
       "1    9.588100         1775  \n",
       "2   13.911338         6404  \n",
       "3   12.007906        25167  \n",
       "4   10.239482         8330  \n",
       "..        ...          ...  \n",
       "95   8.938295        10027  \n",
       "96  14.675909         6918  \n",
       "97   9.889280        15411  \n",
       "98   8.999657         7688  \n",
       "99  15.780628        10202  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stop_pandas = data_stop_stat.toPandas()\n",
    "data_stop_pandas.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "if SAVE_COMPUTATIONS:\n",
    "    data_stop_pandas.to_csv((\"dataset_generated/data_stop/data_stop.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Statistics for each day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = df.withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\")).groupBy(\"stop_name\", \"day_of_week\").agg(F.count(\"train_number\").alias(\"count_stops\")).cache()\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name\", \"day_of_week\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        # F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=[\"stop_name\", \"day_of_week\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_stop_stat = data_stop_stat.filter(col(\"count_stops\") >= MIN_THRESHOLD_STOP)\n",
    "data_stop_stat_pandas = data_stop_stat.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file for each day of week with the statistics\n",
    "if SAVE_COMPUTATIONS:\n",
    "    for day in data_stop_stat_pandas[\"day_of_week\"].unique():\n",
    "        data_stop_stat_pandas[data_stop_stat_pandas[\"day_of_week\"] == day].to_csv((\"dataset_generated/data_stop/data_stop_{}.csv\".format(day)), index=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Statistics for each train type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = df.groupBy(\"stop_name\", \"train_class\").agg(\n",
    "    F.count(\"train_number\").alias(\"count_stops\"),\n",
    "    F.countDistinct(\"date\").alias(\"count_dates\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stop_stat = data_stop.groupBy(\"stop_name\", \"train_class\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=[\"stop_name\", \"train_class\"], how=\"inner\") \\\n",
    "    .filter(col(\"count_dates\") >= MIN_THRESHOLD_DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_stop_stat_pandas = data_stop_stat.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file for each day of week with the statistics\n",
    "if SAVE_COMPUTATIONS:\n",
    "    for train_class in data_stop_stat_pandas[\"train_class\"].unique():\n",
    "        data_stop_stat_pandas[data_stop_stat_pandas[\"train_class\"] == train_class].to_csv((\"dataset_generated/data_stop/data_stop_class_{}.csv\".format(train_class)), index=False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Statistics for each week day and train type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = df.withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\")).groupBy(\"stop_name\", \"train_class\", \"day_of_week\").agg(\n",
    "    F.count(\"train_number\").alias(\"count_stops\"),\n",
    "    F.countDistinct(\"date\").alias(\"count_dates\")    \n",
    ")\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name\", \"train_class\", \"day_of_week\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        # F.count(\"train_id\").alias(\"count_stops\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\")\n",
    "    ).join(stop_counts, on=[\"stop_name\", \"train_class\", \"day_of_week\"], how=\"inner\") \\\n",
    "    .filter(col(\"count_dates\") >= MIN_THRESHOLD_DATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_stop_stat_pandas = data_stop_stat.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each combination of weekday and train_class, create a file with the statistics\n",
    "if SAVE_COMPUTATIONS:\n",
    "    for day in data_stop_stat_pandas[\"day_of_week\"].unique():\n",
    "        for train_class in data_stop_stat_pandas[\"train_class\"].unique():\n",
    "            data_stop_stat_pandas[(data_stop_stat_pandas[\"day_of_week\"] == day) & (data_stop_stat_pandas[\"train_class\"] == train_class)].to_csv((\"dataset_generated/data_stop/data_stop_mix_{}_{}.csv\".format(day, train_class)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if SAVE_COMPUTATIONS: \n",
    "    # zip the folder\n",
    "    shutil.make_archive(\"dataset_generated/data_stop\", 'zip', \"dataset_generated/data_stop\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1d. Statistics for each region\n",
    "\n",
    "Here we aim to extract some insights on train delays for each Italian region. Specifically, we want to compute the average arrival delay for each region, and the statistics used in the previous tasks. We are interested to see those differences for each train class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapper = {\n",
    "    1: \"Lombardia\",\n",
    "    2: \"Liguria\",\n",
    "    3: \"Piemonte\",\n",
    "    4: \"Valle d'Aosta\",\n",
    "    5: \"Lazio\",\n",
    "    6: \"Umbria\",\n",
    "    7: \"Molise\",\n",
    "    8: \"Emilia-Romagna\",\n",
    "    9: \"Trentino-Alto Adige\",\n",
    "    10: \"Friuli-Venezia Giulia\",\n",
    "    11: \"Marche\",\n",
    "    12 : \"Veneto\",\n",
    "    13: \"Toscana\",\n",
    "    14: \"Sicilia\",\n",
    "    15: \"Basilicata\",\n",
    "    16: \"Puglia\",\n",
    "    17: \"Calabria\",\n",
    "    18: \"Campania\",\n",
    "    19: \"Abruzzo\",\n",
    "    20: \"Sardegna\",\n",
    "    21: \"Trentino-Alto Adige\", #actually some stops of province of Trento.....\n",
    "    22: \"Trentino-Alto Adige\", #actually some stops of province of Bolzano.....\n",
    "    None: \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "stops_region = stops.withColumn(\"stop_name_region\", F.udf(lambda x: region_mapper[x], StringType())(\"stop_id_region\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/23 22:32:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "data_stop_first = df.join(stops_region, on=\"stop_name\", how=\"inner\")\n",
    "\n",
    "data_stop = data_stop_first \\\n",
    "    .filter(col(\"stop_arrival_delay\").cast(\"double\").isNotNull()) \\\n",
    "    .withColumn(\"stop_arrival_delay_double\", col(\"stop_arrival_delay\").cast(\"double\")) \\\n",
    "    .drop(\"stop_arrival_delay\") \\\n",
    "    .withColumnRenamed(\"stop_arrival_delay_double\", \"stop_arrival_delay\") \\\n",
    "    .withColumn(\"3m_delay\", col(\"stop_arrival_delay\") > 3)\\\n",
    "    .withColumn(\"5m_delay\", col(\"stop_arrival_delay\") > 5)\\\n",
    "    .withColumn(\"10m_delay\", col(\"stop_arrival_delay\") > 10)\\\n",
    "    .withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"E\"))\\\n",
    "    .withColumn(\"train_id\", concat(col(\"train_class\"), col(\"train_number\"))) \\\n",
    "    .cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = data_stop.groupBy(\"stop_name_region\").agg(\n",
    "    F.count(\"train_id\").alias(\"count_stops\"), \n",
    "    F.countDistinct(\"date\").alias(\"count_dates\")\n",
    ")\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name_region\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        F.countDistinct(\"stop_name\").alias(\"count_stations\"),\n",
    "        F.first(\"stop_id_region\").alias(\"stop_id_region\"),\n",
    "    ).join(stop_counts, on=[\"stop_name_region\"], how=\"inner\") \\\n",
    "    .filter(col(\"count_dates\") >= MIN_THRESHOLD_DATES) \\\n",
    "    .sort(\"stop_id_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if SAVE_COMPUTATIONS:\n",
    "    data_stop_stat.toPandas().to_csv(\"dataset_generated/data_stop_region/data_stop_region.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counts = data_stop.groupBy(\"stop_name_region\", \"train_class\").agg(\n",
    "    F.count(\"train_id\").alias(\"count_stops\"),\n",
    "    F.countDistinct(\"date\").alias(\"count_dates\")\n",
    ")\n",
    "\n",
    "data_stop_stat = data_stop.groupBy(\"stop_name_region\", \"train_class\") \\\n",
    "    .agg(\n",
    "        F.avg(\"stop_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"stop_arrival_delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"train_id\").alias(\"count_trains\"),\n",
    "        F.countDistinct(\"stop_name\").alias(\"count_stations\"),\n",
    "        F.first(\"stop_id_region\").alias(\"stop_id_region\"),\n",
    "    ).join(stop_counts, on=[\"stop_name_region\", \"train_class\"], how=\"inner\") \\\n",
    "    .filter(col(\"count_dates\") >= MIN_THRESHOLD_DATES) \\\n",
    "    .sort(\"stop_id_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if SAVE_COMPUTATIONS:\n",
    "    data_stop_stat_pandas = data_stop_stat.toPandas()\n",
    "\n",
    "    for train_class in data_stop_stat_pandas[\"train_class\"].unique():\n",
    "        data_stop_stat_pandas[data_stop_stat_pandas[\"train_class\"] == train_class].to_csv((\"dataset_generated/data_stop_region/data_stop_region_class_{}.csv\".format(train_class)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the folder\n",
    "if SAVE_COMPUTATIONS:\n",
    "    shutil.make_archive(\"dataset_generated/data_stop_region\", 'zip', \"dataset_generated/data_stop_region\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train statistics\n",
    "For each train, the goal is to store the following information:\n",
    "1. Ordered list of the stations it stopped at\n",
    "2. For each destination: \n",
    "    1. Average arrival delay\n",
    "    2. Median arrival delay\n",
    "    3. % of trains with delay > 3\n",
    "    4. % of trains with delay > 5\n",
    "    5. % of trains with delay > 10\n",
    "    6. Number of days it stopped at the destination\n",
    "\n",
    "To avoid keeping statistics for temporary stops and trains, we remove: \n",
    "- Trains that appeared in the dataset only <10  distinct times\n",
    "- Stops for a train, that appeared less than 20% of the dates that the train appeared in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we add coordinates to each stop, so that we can drop stops for which we don't have the coordinates\n",
    "df_trains = df.join(stops, on=\"stop_name\", how=\"inner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. For each train, we obtain the statistics of each of its stops, removing the stops that appear less than 20% of the dates that the train appeared in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, we have to keep in mind that the same train number can refer to multiple trains, so we also have to group by the destination stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 344:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|train_arrival_stop_name|\n",
      "+-----------------------+\n",
      "|           ROMA TERMINI|\n",
      "|         COMO NORD LAGO|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# this shows that there's multiple trains with the same number\n",
    "df \\\n",
    "    .filter((F.col(\"train_class\") == \"REG\") & (F.col(\"train_number\") == \"4113\"))\\\n",
    "    .select(\"train_arrival_stop_name\")\\\n",
    "    .distinct()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we create a new column with the delay in minutes. For all stops this has to be the arrival_delay apart for the first stop of each train, for which it has to be the departure_delay\n",
    "df_trains1 = df_trains.withColumn(\"delay\", F.when(F.col(\"stop_arrival_delay\") == \"N\", F.col(\"stop_departure_delay\")).otherwise(F.col(\"stop_arrival_delay\")))\n",
    "\n",
    "# second, add a column \"stop_arrival_time\", which is the arrival time at the stop, in the format \"HH:MM\"\n",
    "# if the stop is the first stop of the train, then the stop_arrival_time is the departure time of the train\n",
    "\n",
    "df_trains1 = df_trains1\\\n",
    "    .withColumn(\"stop_time\", F.when(F.col(\"stop_arrival_time\") == 0, F.col(\"stop_departure_time\")).otherwise(F.col(\"stop_arrival_time\"))) \\\n",
    "    .withColumn(\"stop_time\", F.date_format(F.col(\"stop_time\").cast(\"timestamp\"), \"HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the delay to double and add a counter if the train was 3m, 5m, 10m late\n",
    "# add a column with the number of distinct dates for each train\n",
    "# remove the column if the delay cannot be casted to double\n",
    "df_trains2 = df_trains1 \\\n",
    "    .withColumn(\"delay\", F.col(\"delay\").cast(\"double\")) \\\n",
    "    .filter(F.col(\"delay\").isNotNull()) \\\n",
    "    .withColumn(\"3m_delay\", F.when(F.col(\"delay\") >= 3, 1).otherwise(0)) \\\n",
    "    .withColumn(\"5m_delay\", F.when(F.col(\"delay\") >= 5, 1).otherwise(0)) \\\n",
    "    .withColumn(\"10m_delay\", F.when(F.col(\"delay\") >= 10, 1).otherwise(0))\\\n",
    "    .select(\"train_departure_stop_name\", \"train_arrival_stop_name\", \"train_class\", \"train_number\", \"date\", \"stop_id\", \"stop_time\", \"stop_name\", \"stop_name_short\", \"stop_lat\", \"stop_lon\", \"stop_id_region\", \"3m_delay\", \"5m_delay\", \"10m_delay\", \"delay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each train, we obtain the statistics of each of its stops, removing the stops that appear less than 20% of the dates that the train appeared in the dataset\n",
    "# add the stop incremental number\n",
    "df_trains_stat1 = df_trains2.groupBy(\"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\", \"stop_name\") \\\n",
    "    .agg(\n",
    "        F.avg(\"delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.percentile_approx(\"delay\", 0.5).alias(\"median_arrival_delay\"),\n",
    "        F.countDistinct(\"date\").alias(\"count_dates_stop\"),\n",
    "        F.min(\"date\").alias(\"first_date\"),\n",
    "        F.max(\"date\").alias(\"last_date\"),\n",
    "        F.sum(F.col(\"3m_delay\").cast(\"long\")).alias(\"count_3m_delay\"),\n",
    "        F.sum(F.col(\"5m_delay\").cast(\"long\")).alias(\"count_5m_delay\"),\n",
    "        F.sum(F.col(\"10m_delay\").cast(\"long\")).alias(\"count_10m_delay\"),\n",
    "        F.first(\"stop_lat\").alias(\"stop_lat\"),\n",
    "        F.first(\"stop_lon\").alias(\"stop_lon\"),\n",
    "        F.mode(\"stop_time\").alias(\"stop_time\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts how many distinct dates a train apparead in the dataset\n",
    "# and filter out trains that didn't appear at least 4 times a month (12 times in total)\n",
    "MIN_THRESHOLD_TRAIN = 12\n",
    "\n",
    "df_trains_counts = df_trains2.groupBy(\"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(\"date\").alias(\"count_dates_train\")\n",
    "    ) \\\n",
    "    .filter(F.col(\"count_dates_train\") >= MIN_THRESHOLD_TRAIN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store the data we create a mapper from a (train_class, train_number, train_arrival_stop_name) to an id, and we store the information of that train on a file with the name of the id. To do that, we add a column which is a monotonic increasing id, and we use that as the id of that train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 356:======>       (88 + 8) / 200][Stage 358:=====>        (84 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "# df_trains_counts = df_trains_counts \\\n",
    "#    .withColumn(\"train_id\", F.monotonically_increasing_id()) \\\n",
    "#    .cache()\n",
    "\n",
    "\n",
    "# Update, instead of the monotonically increasing Id, we add a concatenation of the train keys\n",
    "\n",
    "\n",
    "df_trains_counts = df_trains_counts \\\n",
    "   .withColumn(\"train_id\", F.concat(F.col(\"train_class\"), F.col(\"train_number\"), F.col(\"train_departure_stop_name\"), F.col(\"train_arrival_stop_name\"))) \\\n",
    "   .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets and filter out stations that appear in less than 20% of the dates\n",
    "df_trains_stat2 = df_trains_stat1.join(df_trains_counts, on=[\"train_class\", \"train_number\", \"train_arrival_stop_name\", \"train_departure_stop_name\"], how=\"inner\") \\\n",
    "    .filter(F.col(\"count_dates_stop\") >= F.col(\"count_dates_train\") * 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# add the stop incremental number\n",
    "df_trains_stat3 = df_trains_stat2 \\\n",
    "    .withColumn(\"stop_number\", F.row_number().over(Window.partitionBy(\"train_id\").orderBy(\"stop_time\"))) \\\n",
    "    .sort(\"train_id\", \"stop_number\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 356:======>       (96 + 8) / 200][Stage 358:=====>        (84 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "# remove trains without class and order by \"count_dates_train\"\n",
    "df_trains_stat3 = df_trains_stat3.filter(F.col(\"train_class\")!= \"\")\n",
    "df_trains_stat3 = df_trains_stat3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now we store df_trains_stat2 in a csv file, partitioned by the column \"train_id\". \n",
    "# Each \"train_id\" should have a single file, and each file should contain the statistics of all the stops of the train\n",
    "\n",
    "# remove folder if it already exists\n",
    "\n",
    "if SAVE_COMPUTATIONS : \n",
    "    import shutil\n",
    "    # if os.path.exists(\"dataset_generated/data_train_stat\"): \n",
    "    shutil.rmtree(\"dataset_generated/data_train_stat\")\n",
    "\n",
    "\n",
    "    df_trains_stat3 \\\n",
    "        .repartition(\"train_id\") \\\n",
    "        .write \\\n",
    "        .partitionBy(\"train_id\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"dataset_generated/data_train_stat/data_train_stat.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_COMPUTATIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the previous cell generates a folder with a lot of subfolders, one for each train. \n",
    "# each folder name is called \"train_id=XXXXX\", where XXXXX is the id of the train\n",
    "# we want to generate a new folder with a single file for each train, where the file name is the train id\n",
    "# the file should contain the statistics of all the stops of the train\n",
    "\n",
    "# remove folder if it already exists\n",
    "if SAVE_COMPUTATIONS :\n",
    "    import shutil\n",
    "\n",
    "    import os\n",
    "\n",
    "    if os.path.exists(\"dataset_generated/data_train_stat_single_file\") :\n",
    "        shutil.rmtree(\"dataset_generated/data_train_stat_single_file\")\n",
    "\n",
    "    os.mkdir(\"dataset_generated/data_train_stat_single_file\")\n",
    "\n",
    "    import glob\n",
    "    import shutil\n",
    "\n",
    "    # get all the subfolders\n",
    "    subfolders = [f.path for f in os.scandir(\"dataset_generated/data_train_stat/data_train_stat.csv\") if f.is_dir() ] \n",
    "\n",
    "    # for each subfolder, get the csv file and rename it\n",
    "    for subfolder in subfolders : \n",
    "        # get the csv file\n",
    "        csv_file = glob.glob(subfolder + \"/*.csv\")[0]\n",
    "\n",
    "        # get the train id\n",
    "        train_id = subfolder.split(\"=\")[1]\n",
    "\n",
    "        # rename the file\n",
    "        shutil.copy(csv_file, \"dataset_generated/data_train_stat_single_file/\" + train_id + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we zip the folder data_train_stat_single_file and we put the zip in data_train_stat\n",
    "if SAVE_COMPUTATIONS :\n",
    "    import shutil\n",
    "    shutil.make_archive(\"dataset_generated/data_train_stat/data_train_stat\", 'zip', \"dataset_generated/data_train_stat_single_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "# df_trains_stat3 = spark.read.option(\"header\", \"true\").csv(\"dataset_generated/data_train_stat/data_train_stat.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to elaborate some general stastistics about a train, and store a dataset which is an index of the train ids, and the statistics of that train.\n",
    "\n",
    "The aggregated statistics that we want to store are: \n",
    "1. Average arrival delay at each destination\n",
    "2. Median arrival delay at each destination\n",
    "3. % of trains with delay > 3 at each destination\n",
    "4. % of trains with delay > 5 at each destination\n",
    "5. % of trains with delay > 10 at each destination\n",
    "6. Number of days the train ran\n",
    "7. Number of stops of the train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trains_stat3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to elaborate some general stastistics about a train, and store a dataset which is an index of the train ids, and the statistics of that train.\n",
    "\n",
    "# The aggregated statistics that we want to store are: \n",
    "# 1. Average arrival delay at each destination\n",
    "# 2. Median arrival delay at each destination\n",
    "# 3. % of trains with delay > 3 at each destination\n",
    "# 4. % of trains with delay > 5 at each destination\n",
    "# 5. % of trains with delay > 10 at each destination\n",
    "# 6. Number of days the train ran\n",
    "# 7. Number of stops of the train\n",
    "\n",
    "\n",
    "df_trains_stat4 = df_trains_stat3 \\\n",
    "    .groupBy(\"train_id\", \"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\") \\\n",
    "    .agg(\n",
    "        F.avg(\"avg_arrival_delay\").alias(\"avg_arrival_delay\"),\n",
    "        F.avg(\"median_arrival_delay\").alias(\"median_arrival_delay\"),\n",
    "        (F.avg(\"count_3m_delay\") / F.first(\"count_dates_train\")).alias(\"perc_3m_delay\"),\n",
    "        (F.avg(\"count_5m_delay\") / F.first(\"count_dates_train\")).alias(\"perc_5m_delay\"),\n",
    "        (F.avg(\"count_10m_delay\") / F.first(\"count_dates_train\")).alias(\"perc_10m_delay\"),\n",
    "        F.first(\"count_dates_train\").alias(\"count_dates_train\"),\n",
    "        F.countDistinct(\"stop_name\").alias(\"count_stops_train\"),\n",
    "        F.first(\"first_date\").alias(\"first_date\"),\n",
    "        F.first(\"last_date\").alias(\"last_date\"),\n",
    "        F.min(\"stop_time\").alias(\"departure_time\"),\n",
    "        F.max(\"stop_time\").alias(\"arrival_time\"),\n",
    "    ) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trains_stat4_pandas = df_trains_stat4.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>train_class</th>\n",
       "      <th>train_number</th>\n",
       "      <th>train_departure_stop_name</th>\n",
       "      <th>train_arrival_stop_name</th>\n",
       "      <th>avg_arrival_delay</th>\n",
       "      <th>median_arrival_delay</th>\n",
       "      <th>perc_3m_delay</th>\n",
       "      <th>perc_5m_delay</th>\n",
       "      <th>perc_10m_delay</th>\n",
       "      <th>count_dates_train</th>\n",
       "      <th>count_stops_train</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>arrival_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FR9527MILANO CENTRALENAPOLI CENTRALE</td>\n",
       "      <td>FR</td>\n",
       "      <td>9527</td>\n",
       "      <td>MILANO CENTRALE</td>\n",
       "      <td>NAPOLI CENTRALE</td>\n",
       "      <td>17.491758</td>\n",
       "      <td>11.125000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>2023-10-23</td>\n",
       "      <td>10:15</td>\n",
       "      <td>15:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REG10021MI.P.GENOVAMORTARA</td>\n",
       "      <td>REG</td>\n",
       "      <td>10021</td>\n",
       "      <td>MI.P.GENOVA</td>\n",
       "      <td>MORTARA</td>\n",
       "      <td>5.922711</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.354663</td>\n",
       "      <td>0.205853</td>\n",
       "      <td>288</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>06:42</td>\n",
       "      <td>07:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REG1139M N CADORNACOMO NORD LAGO</td>\n",
       "      <td>REG</td>\n",
       "      <td>1139</td>\n",
       "      <td>M N CADORNA</td>\n",
       "      <td>COMO NORD LAGO</td>\n",
       "      <td>0.236287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-12-23</td>\n",
       "      <td>12:32</td>\n",
       "      <td>12:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REG11818AOSTAIVREA</td>\n",
       "      <td>REG</td>\n",
       "      <td>11818</td>\n",
       "      <td>AOSTA</td>\n",
       "      <td>IVREA</td>\n",
       "      <td>2.311655</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.382601</td>\n",
       "      <td>0.178632</td>\n",
       "      <td>0.031672</td>\n",
       "      <td>296</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>07:04</td>\n",
       "      <td>08:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REG1224NOVARA NORDM N CADORNA</td>\n",
       "      <td>REG</td>\n",
       "      <td>1224</td>\n",
       "      <td>NOVARA NORD</td>\n",
       "      <td>M N CADORNA</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113248</td>\n",
       "      <td>0.068376</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>234</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-12-23</td>\n",
       "      <td>08:08</td>\n",
       "      <td>08:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>REG12683ROMA TERMINILATINA</td>\n",
       "      <td>REG</td>\n",
       "      <td>12683</td>\n",
       "      <td>ROMA TERMINI</td>\n",
       "      <td>LATINA</td>\n",
       "      <td>2.504683</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.283977</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.078684</td>\n",
       "      <td>233</td>\n",
       "      <td>6</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>17:06</td>\n",
       "      <td>17:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>REG12917MESSINA CENTRALEPATTI-SAN PIERO PATTI</td>\n",
       "      <td>REG</td>\n",
       "      <td>12917</td>\n",
       "      <td>MESSINA CENTRALE</td>\n",
       "      <td>PATTI-SAN PIERO PATTI</td>\n",
       "      <td>2.108881</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.356557</td>\n",
       "      <td>0.154827</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>244</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>21:23</td>\n",
       "      <td>22:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>REG12991MESSINA CENTRALECATANIA AER. FONTANAROSSA</td>\n",
       "      <td>REG</td>\n",
       "      <td>12991</td>\n",
       "      <td>MESSINA CENTRALE</td>\n",
       "      <td>CATANIA AER. FONTANAROSSA</td>\n",
       "      <td>1.888889</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>2023-12-12</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>19:27</td>\n",
       "      <td>21:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>REG16020VENEZIA SANTA LUCIATRIESTE CENTRALE</td>\n",
       "      <td>REG</td>\n",
       "      <td>16020</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>TRIESTE CENTRALE</td>\n",
       "      <td>1.934426</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.239195</td>\n",
       "      <td>0.116990</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>19:01</td>\n",
       "      <td>22:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REG16707BOLZANOTRENTO</td>\n",
       "      <td>REG</td>\n",
       "      <td>16707</td>\n",
       "      <td>BOLZANO</td>\n",
       "      <td>TRENTO</td>\n",
       "      <td>5.401639</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.385246</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>19:41</td>\n",
       "      <td>19:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            train_id train_class train_number  \\\n",
       "0               FR9527MILANO CENTRALENAPOLI CENTRALE          FR         9527   \n",
       "1                         REG10021MI.P.GENOVAMORTARA         REG        10021   \n",
       "2                   REG1139M N CADORNACOMO NORD LAGO         REG         1139   \n",
       "3                                 REG11818AOSTAIVREA         REG        11818   \n",
       "4                      REG1224NOVARA NORDM N CADORNA         REG         1224   \n",
       "5                         REG12683ROMA TERMINILATINA         REG        12683   \n",
       "6      REG12917MESSINA CENTRALEPATTI-SAN PIERO PATTI         REG        12917   \n",
       "7  REG12991MESSINA CENTRALECATANIA AER. FONTANAROSSA         REG        12991   \n",
       "8        REG16020VENEZIA SANTA LUCIATRIESTE CENTRALE         REG        16020   \n",
       "9                              REG16707BOLZANOTRENTO         REG        16707   \n",
       "\n",
       "  train_departure_stop_name    train_arrival_stop_name  avg_arrival_delay  \\\n",
       "0           MILANO CENTRALE            NAPOLI CENTRALE          17.491758   \n",
       "1               MI.P.GENOVA                    MORTARA           5.922711   \n",
       "2               M N CADORNA             COMO NORD LAGO           0.236287   \n",
       "3                     AOSTA                      IVREA           2.311655   \n",
       "4               NOVARA NORD                M N CADORNA           0.811966   \n",
       "5              ROMA TERMINI                     LATINA           2.504683   \n",
       "6          MESSINA CENTRALE      PATTI-SAN PIERO PATTI           2.108881   \n",
       "7          MESSINA CENTRALE  CATANIA AER. FONTANAROSSA           1.888889   \n",
       "8       VENEZIA SANTA LUCIA           TRIESTE CENTRALE           1.934426   \n",
       "9                   BOLZANO                     TRENTO           5.401639   \n",
       "\n",
       "   median_arrival_delay  perc_3m_delay  perc_5m_delay  perc_10m_delay  \\\n",
       "0             11.125000       0.714286       0.571429        0.410714   \n",
       "1              2.571429       0.486111       0.354663        0.205853   \n",
       "2              0.000000       0.037975       0.012658        0.000000   \n",
       "3              1.625000       0.382601       0.178632        0.031672   \n",
       "4              0.000000       0.113248       0.068376        0.029915   \n",
       "5              0.833333       0.283977       0.166667        0.078684   \n",
       "6              1.444444       0.356557       0.154827        0.018670   \n",
       "7              1.055556       0.307692       0.192308        0.029915   \n",
       "8              0.863636       0.239195       0.116990        0.039493   \n",
       "9              4.000000       0.704918       0.385246        0.114754   \n",
       "\n",
       "   count_dates_train  count_stops_train  first_date   last_date  \\\n",
       "0                 14                  8  2023-08-07  2023-10-23   \n",
       "1                288                  7  2023-01-02  2023-12-29   \n",
       "2                237                  1  2023-01-02  2023-12-23   \n",
       "3                296                  8  2023-01-02  2023-12-30   \n",
       "4                234                  4  2023-01-02  2023-12-23   \n",
       "5                233                  6  2023-01-02  2023-12-28   \n",
       "6                244                  9  2023-01-02  2023-12-29   \n",
       "7                 13                 18  2023-12-12  2023-12-29   \n",
       "8                 61                 22  2023-01-01  2023-12-31   \n",
       "9                 61                  2  2023-01-01  2023-12-31   \n",
       "\n",
       "  departure_time arrival_time  \n",
       "0          10:15        15:12  \n",
       "1          06:42        07:35  \n",
       "2          12:32        12:32  \n",
       "3          07:04        08:22  \n",
       "4          08:08        08:22  \n",
       "5          17:06        17:52  \n",
       "6          21:23        22:28  \n",
       "7          19:27        21:26  \n",
       "8          19:01        22:44  \n",
       "9          19:41        19:54  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trains_stat4_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: dataset_generated/data_train_index.csv (deflated 68%)\n"
     ]
    }
   ],
   "source": [
    "if SAVE_COMPUTATIONS :    \n",
    "    # store the file in a csv file  \n",
    "    df_trains_stat4_pandas.to_csv(\"dataset_generated/data_train_index.csv\", index=False)\n",
    "\n",
    "    # zip the file\n",
    "    !zip dataset_generated/data_train_index.zip dataset_generated/data_train_index.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we store the data in a better format, which is a folder in which each file is a train, and the filename is the train id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# if the folder already exists, delete it\n",
    "if os.path.exists(\"renamed_csv_files\"):\n",
    "    shutil.rmtree(\"renamed_csv_files\")\n",
    "\n",
    "# create a new directory to store the renamed CSV files\n",
    "if not os.path.exists(\"renamed_csv_files\"):\n",
    "    os.mkdir(\"renamed_csv_files\")\n",
    "\n",
    "# loop through all directories in the \"data_train_stat/data_train_stat.csv\" directory that start with \"train_id=\"\n",
    "for dirpath, dirnames, filenames in os.walk(\"dataset_generated/data_train_stat/data_train_stat.csv\"):\n",
    "    for dirname in dirnames:\n",
    "        if dirname.startswith(\"train_id=\"):\n",
    "            # extract the ID from the directory name\n",
    "            id = dirname.split(\"=\")[1]\n",
    "            # loop through all CSV files in the directory\n",
    "            for filename in os.listdir(os.path.join(dirpath, dirname)):\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    # rename the file to \"[ID].csv\" and move it to the \"renamed_csv_files\" directory\n",
    "                    src_path = os.path.join(dirpath, dirname, filename)\n",
    "                    dst_path = os.path.join(\"renamed_csv_files\", f\"{id}.csv\")\n",
    "                    shutil.copy(src_path, dst_path)\n",
    "\n",
    "\n",
    "# create a zip file containing the \"renamed_csv_files\" directory, the files have to be in a directory when unzipped\n",
    "shutil.make_archive(\"renamed_csv_files\", \"zip\", \"renamed_csv_files\")\n",
    "\n",
    "# delete the \"renamed_csv_files\" directory\n",
    "shutil.rmtree(\"renamed_csv_files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Path of trains over a map\n",
    "Now that we have computed a dataset with the statistics for each train, in which we managed to extract the timetable, we can use it to plot the path of each train on a map. \n",
    "\n",
    "The task of matching information of a train, with its exact journey on the railway is a complex task. The algorithm developed by [Bast and Brosi (2019)](https://ad-publications.cs.uni-freiburg.de/SIGSPATIAL_Sparse%20map%20matching%202018.pdf) matches the GTFS schedule of a train with the OpenStreetMap railway network. Thankfully, they published the code of this algorithm on GitHub. Their library takes a GTFS schedule of a train and a railway network in OpenStreetMap format and returns the most likely journey of the train on the railway network in `shapefile` format. GTFS is a standard format for public transport schedules, promoted by Google.\n",
    "\n",
    "First, we need to generate a GTFS timetable for our trains. Then, we can run the algorithm a produce a shapefile for each train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trains_timetable = df_trains_stat3 \\\n",
    "    .select(\"train_id\", \"train_class\", \"train_number\", \"train_departure_stop_name\", \"train_arrival_stop_name\", \"stop_number\", \"stop_name\", \"stop_time\", \"stop_lat\", \"stop_lon\")\n",
    "\n",
    "# need to join with the stops to get stop_id that we have lost on the way\n",
    "trains_timetable = trains_timetable \\\n",
    "    .join(stops, on=[\"stop_name\", \"stop_lat\", \"stop_lon\"], how=\"inner\")\n",
    "\n",
    "trains_timetable_pandas = trains_timetable.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>train_id</th>\n",
       "      <th>train_class</th>\n",
       "      <th>train_number</th>\n",
       "      <th>train_departure_stop_name</th>\n",
       "      <th>train_arrival_stop_name</th>\n",
       "      <th>stop_number</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_name_short</th>\n",
       "      <th>stop_id_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>45.441569</td>\n",
       "      <td>12.320882</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>1</td>\n",
       "      <td>15:35</td>\n",
       "      <td>S02593</td>\n",
       "      <td>Venezia S.Lucia</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VENEZIA MESTRE</td>\n",
       "      <td>45.482123</td>\n",
       "      <td>12.232069</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>2</td>\n",
       "      <td>15:45</td>\n",
       "      <td>S02589</td>\n",
       "      <td>Venezia Mestre</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PADOVA</td>\n",
       "      <td>45.417846</td>\n",
       "      <td>11.880794</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>3</td>\n",
       "      <td>16:01</td>\n",
       "      <td>S02581</td>\n",
       "      <td>Padova</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VICENZA</td>\n",
       "      <td>45.541102</td>\n",
       "      <td>11.540762</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>4</td>\n",
       "      <td>16:22</td>\n",
       "      <td>S02446</td>\n",
       "      <td>Vicenza</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VERONA PORTA NUOVA</td>\n",
       "      <td>45.428603</td>\n",
       "      <td>10.982377</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>5</td>\n",
       "      <td>16:50</td>\n",
       "      <td>S02430</td>\n",
       "      <td>Verona P.Nuova</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ROVERETO</td>\n",
       "      <td>45.890985</td>\n",
       "      <td>11.033518</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>6</td>\n",
       "      <td>17:41</td>\n",
       "      <td>S02044</td>\n",
       "      <td>Rovereto</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRENTO</td>\n",
       "      <td>46.072414</td>\n",
       "      <td>11.118941</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>7</td>\n",
       "      <td>17:57</td>\n",
       "      <td>S02038</td>\n",
       "      <td>Trento</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRESSANONE</td>\n",
       "      <td>46.710088</td>\n",
       "      <td>11.649677</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>8</td>\n",
       "      <td>19:02</td>\n",
       "      <td>S02014</td>\n",
       "      <td>Bressanone</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>47.004117</td>\n",
       "      <td>11.505711</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>9</td>\n",
       "      <td>19:48</td>\n",
       "      <td>S02001</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>47.004117</td>\n",
       "      <td>11.505711</td>\n",
       "      <td>EC1281BRENNEROVENEZIA SANTA LUCIA</td>\n",
       "      <td>EC</td>\n",
       "      <td>1281</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>1</td>\n",
       "      <td>10:10</td>\n",
       "      <td>S02001</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             stop_name   stop_lat   stop_lon  \\\n",
       "0  VENEZIA SANTA LUCIA  45.441569  12.320882   \n",
       "1       VENEZIA MESTRE  45.482123  12.232069   \n",
       "2               PADOVA  45.417846  11.880794   \n",
       "3              VICENZA  45.541102  11.540762   \n",
       "4   VERONA PORTA NUOVA  45.428603  10.982377   \n",
       "5             ROVERETO  45.890985  11.033518   \n",
       "6               TRENTO  46.072414  11.118941   \n",
       "7           BRESSANONE  46.710088  11.649677   \n",
       "8             BRENNERO  47.004117  11.505711   \n",
       "9             BRENNERO  47.004117  11.505711   \n",
       "\n",
       "                            train_id train_class train_number  \\\n",
       "0  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "1  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "2  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "3  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "4  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "5  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "6  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "7  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "8  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "9  EC1281BRENNEROVENEZIA SANTA LUCIA          EC         1281   \n",
       "\n",
       "  train_departure_stop_name train_arrival_stop_name  stop_number stop_time  \\\n",
       "0       VENEZIA SANTA LUCIA                BRENNERO            1     15:35   \n",
       "1       VENEZIA SANTA LUCIA                BRENNERO            2     15:45   \n",
       "2       VENEZIA SANTA LUCIA                BRENNERO            3     16:01   \n",
       "3       VENEZIA SANTA LUCIA                BRENNERO            4     16:22   \n",
       "4       VENEZIA SANTA LUCIA                BRENNERO            5     16:50   \n",
       "5       VENEZIA SANTA LUCIA                BRENNERO            6     17:41   \n",
       "6       VENEZIA SANTA LUCIA                BRENNERO            7     17:57   \n",
       "7       VENEZIA SANTA LUCIA                BRENNERO            8     19:02   \n",
       "8       VENEZIA SANTA LUCIA                BRENNERO            9     19:48   \n",
       "9                  BRENNERO     VENEZIA SANTA LUCIA            1     10:10   \n",
       "\n",
       "  stop_id  stop_name_short  stop_id_region  \n",
       "0  S02593  Venezia S.Lucia            12.0  \n",
       "1  S02589   Venezia Mestre            12.0  \n",
       "2  S02581           Padova            12.0  \n",
       "3  S02446          Vicenza            12.0  \n",
       "4  S02430   Verona P.Nuova            12.0  \n",
       "5  S02044         Rovereto            21.0  \n",
       "6  S02038           Trento            21.0  \n",
       "7  S02014       Bressanone             9.0  \n",
       "8  S02001         BRENNERO            22.0  \n",
       "9  S02001         BRENNERO            22.0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains_timetable_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EC', 'FA', 'FB', 'FR', 'IC', 'ICN', 'REG'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at distinct values of train_class\n",
    "trains_timetable_pandas[\"train_class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# delete folder if it exists\n",
    "import shutil\n",
    "if os.path.exists(\"gtfs\"):\n",
    "    shutil.rmtree(\"gtfs\")\n",
    "\n",
    "# create folder gtfs \n",
    "if not os.path.exists(\"gtfs\"):\n",
    "    os.mkdir(\"gtfs\")\n",
    "\n",
    "# 1. Create the agency.txt file\n",
    "agency = {\n",
    "    \"agency_id\": \"1\",\n",
    "    \"agency_name\": \"Trenitalia\",\n",
    "    \"agency_url\": \"https://www.trenitalia.com\",\n",
    "    \"agency_timezone\": \"Europe/Rome\",\n",
    "    \"agency_lang\": \"it\",\n",
    "    \"agency_phone\": \"\"\n",
    "}\n",
    "agency = pd.DataFrame(agency, index=[0])\n",
    "agency.to_csv(\"gtfs/agency.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create routes.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#routestxt\n",
    "\n",
    "\n",
    "routes = pd.DataFrame()\n",
    "\n",
    "# get unique tuples (train_id, train_class, train_number)\n",
    "train_ids = trains_timetable_pandas[[\"train_id\", \"train_class\", \"train_number\"]].drop_duplicates()\n",
    "\n",
    "routes[\"route_id\"] = train_ids[\"train_id\"]\n",
    "routes[\"route_short_name\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "routes[\"route_long_name\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "\n",
    "color_mapper = {\n",
    "    \"IC\": \"E0A434\",\n",
    "    \"REG\": \"036864\",\n",
    "    \"ICN\": \"E0A434\",\n",
    "    \"AV\": \"DC263B\",\n",
    "    \"EC\": \"DC263B\",\n",
    "    \"FR\": \"DC263B\",\n",
    "    \"FA\": \"DC263B\",\n",
    "    \"FB\": \"DC263B\",\n",
    "}\n",
    "\n",
    "routes[\"agency_id\"] = \"1\"\n",
    "routes[\"route_type\"] = \"2\" # 2 is train\n",
    "\n",
    "routes[\"route_color\"] = routes[\"route_short_name\"].apply(lambda x: color_mapper[x.split(\" \")[0]])\n",
    "\n",
    "routes.to_csv(\"gtfs/routes.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create stops.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#stopstxt\n",
    "stops_gtfs = trains_timetable_pandas[[\"stop_id\", \"stop_name\", \"stop_lat\", \"stop_lon\"]].drop_duplicates()\n",
    "stops_gtfs.to_csv(\"gtfs/stops.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tough one, create calendar.txt and trips.txt\n",
    "# calendar describes the span of a service\n",
    "# trips describes the service for a particular route\n",
    "\n",
    "# 4a: create calendar.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#calendartxt\n",
    "calendar = {\n",
    "    # we just create a service that runs every day for the whole year\n",
    "    \"service_id\": \"1\",\n",
    "    \"monday\": \"1\",\n",
    "    \"tuesday\": \"1\",\n",
    "    \"wednesday\": \"1\",\n",
    "    \"thursday\": \"1\",\n",
    "    \"friday\": \"1\",\n",
    "    \"saturday\": \"1\",\n",
    "    \"sunday\": \"1\",\n",
    "    \"start_date\": \"19000101\",\n",
    "    \"end_date\": \"21000101\",\n",
    "}\n",
    "calendar = pd.DataFrame(calendar, index=[0])\n",
    "calendar.to_csv(\"gtfs/calendar.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: create trips.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#tripstxt\n",
    "# trips = {\n",
    "#     \"route_id\": \"1\",\n",
    "#     \"service_id\": \"1\",\n",
    "#     \"trip_id\": \"1\",\n",
    "#     \"trip_headsign\": query_7[\"train_class\"] + \" \" + query_7[\"train_number\"],\n",
    "#     \"trip_short_name\": query_7[\"train_class\"] + \" \" + query_7[\"train_number\"],\n",
    "#     \"direction_id\": \"\",\n",
    "#     \"block_id\": \"\",\n",
    "#     \"shape_id\": \"\",\n",
    "#     \"wheelchair_accessible\": \"\",\n",
    "#     \"bikes_allowed\": \"\",\n",
    "# }\n",
    "\n",
    "# trips = pd.DataFrame(trips, index=[0])\n",
    "\n",
    "trips = pd.DataFrame()\n",
    "\n",
    "trips[\"route_id\"] = train_ids[\"train_id\"]\n",
    "trips[\"service_id\"] = \"1\"\n",
    "trips[\"trip_id\"] = train_ids[\"train_id\"]\n",
    "trips[\"trip_headsign\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "trips[\"trip_short_name\"] = train_ids[\"train_class\"] + \" \" + train_ids[\"train_number\"]\n",
    "trips[\"direction_id\"] = \"\"\n",
    "trips[\"block_id\"] = \"\"\n",
    "trips[\"shape_id\"] = \"\"\n",
    "trips[\"wheelchair_accessible\"] = \"\"\n",
    "trips[\"bikes_allowed\"] = \"\"\n",
    "\n",
    "\n",
    "trips.to_csv(\"gtfs/trips.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>train_id</th>\n",
       "      <th>train_class</th>\n",
       "      <th>train_number</th>\n",
       "      <th>train_departure_stop_name</th>\n",
       "      <th>train_arrival_stop_name</th>\n",
       "      <th>stop_number</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_name_short</th>\n",
       "      <th>stop_id_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>45.441569</td>\n",
       "      <td>12.320882</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>1</td>\n",
       "      <td>15:35</td>\n",
       "      <td>S02593</td>\n",
       "      <td>Venezia S.Lucia</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VENEZIA MESTRE</td>\n",
       "      <td>45.482123</td>\n",
       "      <td>12.232069</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>2</td>\n",
       "      <td>15:45</td>\n",
       "      <td>S02589</td>\n",
       "      <td>Venezia Mestre</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PADOVA</td>\n",
       "      <td>45.417846</td>\n",
       "      <td>11.880794</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>3</td>\n",
       "      <td>16:01</td>\n",
       "      <td>S02581</td>\n",
       "      <td>Padova</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VICENZA</td>\n",
       "      <td>45.541102</td>\n",
       "      <td>11.540762</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>4</td>\n",
       "      <td>16:22</td>\n",
       "      <td>S02446</td>\n",
       "      <td>Vicenza</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VERONA PORTA NUOVA</td>\n",
       "      <td>45.428603</td>\n",
       "      <td>10.982377</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>5</td>\n",
       "      <td>16:50</td>\n",
       "      <td>S02430</td>\n",
       "      <td>Verona P.Nuova</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ROVERETO</td>\n",
       "      <td>45.890985</td>\n",
       "      <td>11.033518</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>6</td>\n",
       "      <td>17:41</td>\n",
       "      <td>S02044</td>\n",
       "      <td>Rovereto</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRENTO</td>\n",
       "      <td>46.072414</td>\n",
       "      <td>11.118941</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>7</td>\n",
       "      <td>17:57</td>\n",
       "      <td>S02038</td>\n",
       "      <td>Trento</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRESSANONE</td>\n",
       "      <td>46.710088</td>\n",
       "      <td>11.649677</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>8</td>\n",
       "      <td>19:02</td>\n",
       "      <td>S02014</td>\n",
       "      <td>Bressanone</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>47.004117</td>\n",
       "      <td>11.505711</td>\n",
       "      <td>EC1280VENEZIA SANTA LUCIABRENNERO</td>\n",
       "      <td>EC</td>\n",
       "      <td>1280</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>9</td>\n",
       "      <td>19:48</td>\n",
       "      <td>S02001</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>47.004117</td>\n",
       "      <td>11.505711</td>\n",
       "      <td>EC1281BRENNEROVENEZIA SANTA LUCIA</td>\n",
       "      <td>EC</td>\n",
       "      <td>1281</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>VENEZIA SANTA LUCIA</td>\n",
       "      <td>1</td>\n",
       "      <td>10:10</td>\n",
       "      <td>S02001</td>\n",
       "      <td>BRENNERO</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             stop_name   stop_lat   stop_lon  \\\n",
       "0  VENEZIA SANTA LUCIA  45.441569  12.320882   \n",
       "1       VENEZIA MESTRE  45.482123  12.232069   \n",
       "2               PADOVA  45.417846  11.880794   \n",
       "3              VICENZA  45.541102  11.540762   \n",
       "4   VERONA PORTA NUOVA  45.428603  10.982377   \n",
       "5             ROVERETO  45.890985  11.033518   \n",
       "6               TRENTO  46.072414  11.118941   \n",
       "7           BRESSANONE  46.710088  11.649677   \n",
       "8             BRENNERO  47.004117  11.505711   \n",
       "9             BRENNERO  47.004117  11.505711   \n",
       "\n",
       "                            train_id train_class train_number  \\\n",
       "0  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "1  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "2  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "3  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "4  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "5  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "6  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "7  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "8  EC1280VENEZIA SANTA LUCIABRENNERO          EC         1280   \n",
       "9  EC1281BRENNEROVENEZIA SANTA LUCIA          EC         1281   \n",
       "\n",
       "  train_departure_stop_name train_arrival_stop_name  stop_number stop_time  \\\n",
       "0       VENEZIA SANTA LUCIA                BRENNERO            1     15:35   \n",
       "1       VENEZIA SANTA LUCIA                BRENNERO            2     15:45   \n",
       "2       VENEZIA SANTA LUCIA                BRENNERO            3     16:01   \n",
       "3       VENEZIA SANTA LUCIA                BRENNERO            4     16:22   \n",
       "4       VENEZIA SANTA LUCIA                BRENNERO            5     16:50   \n",
       "5       VENEZIA SANTA LUCIA                BRENNERO            6     17:41   \n",
       "6       VENEZIA SANTA LUCIA                BRENNERO            7     17:57   \n",
       "7       VENEZIA SANTA LUCIA                BRENNERO            8     19:02   \n",
       "8       VENEZIA SANTA LUCIA                BRENNERO            9     19:48   \n",
       "9                  BRENNERO     VENEZIA SANTA LUCIA            1     10:10   \n",
       "\n",
       "  stop_id  stop_name_short  stop_id_region  \n",
       "0  S02593  Venezia S.Lucia            12.0  \n",
       "1  S02589   Venezia Mestre            12.0  \n",
       "2  S02581           Padova            12.0  \n",
       "3  S02446          Vicenza            12.0  \n",
       "4  S02430   Verona P.Nuova            12.0  \n",
       "5  S02044         Rovereto            21.0  \n",
       "6  S02038           Trento            21.0  \n",
       "7  S02014       Bressanone             9.0  \n",
       "8  S02001         BRENNERO            22.0  \n",
       "9  S02001         BRENNERO            22.0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains_timetable_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create stop_times.txt\n",
    "# docs: https://gtfs.org/schedule/reference/#stop_timestxt\n",
    "# convert stop_departure_time to HH:MM:SS using pyspark function\n",
    "\n",
    "# stop_times: pd.DataFrame = train_data_day_df[[\"stop_id\", \"stop_departure_time\", \"stop_arrival_time\"]] \\\n",
    "#     .rename(columns={\"stop_id\": \"stop_id\", \"stop_departure_time\": \"departure_time\", \"stop_arrival_time\": \"arrival_time\"})\n",
    "\n",
    "\n",
    "# def convert_to_hh_mm_ss(time):\n",
    "    \n",
    "                                                                                                            \n",
    "# # add column with stop_sequence\n",
    "# stop_times[\"stop_sequence\"] = stop_times.index + 1\n",
    "# # add column with trip_id\n",
    "# stop_times[\"trip_id\"] = \"1\"\n",
    "# # put departure time of the last stop equal to the arrival time\n",
    "# stop_times.loc[stop_times.index[-1], \"departure_time\"] = stop_times.loc[stop_times.index[-1], \"arrival_time\"]\n",
    "# store it\n",
    "\n",
    "stop_times_gtfs = pd.DataFrame()\n",
    "\n",
    "stop_times_gtfs[\"trip_id\"] = trains_timetable_pandas[\"train_id\"]\n",
    "stop_times_gtfs[\"arrival_time\"] = trains_timetable_pandas[\"stop_time\"].apply(lambda x: x + \":00\")\n",
    "stop_times_gtfs[\"departure_time\"] = trains_timetable_pandas[\"stop_time\"].apply(lambda x: x + \":00\")\n",
    "stop_times_gtfs[\"stop_id\"] = trains_timetable_pandas[\"stop_id\"]\n",
    "stop_times_gtfs[\"stop_sequence\"] = trains_timetable_pandas[\"stop_number\"]\n",
    "\n",
    "stop_times_gtfs.to_csv(\"gtfs/stop_times.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a zip using libzip\n",
    "import zipfile\n",
    "zf = zipfile.ZipFile('gtfs.zip', mode='w')\n",
    "try:\n",
    "    zf.write(\"gtfs/agency.txt\")\n",
    "    zf.write(\"gtfs/calendar.txt\")\n",
    "    zf.write(\"gtfs/routes.txt\")\n",
    "    zf.write(\"gtfs/stops.txt\")\n",
    "    zf.write(\"gtfs/stop_times.txt\")\n",
    "    zf.write(\"gtfs/trips.txt\")\n",
    "finally:\n",
    "    zf.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the algorithm\n",
    "We downloaded and installed pfaedle as well as an OSM dump for Italy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pfaedle -x ~/Downloads/italy-latest.osm {}\"\"\".format(os.path.join(os.getcwd(), \"gtfs\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the shapes in the \"shapes.txt\" file, we want to separate all those files into one file for each train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_id</th>\n",
       "      <th>shape_pt_lat</th>\n",
       "      <th>shape_pt_lon</th>\n",
       "      <th>shape_pt_sequence</th>\n",
       "      <th>shape_dist_traveled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.760090</td>\n",
       "      <td>10.996771</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.760235</td>\n",
       "      <td>10.996886</td>\n",
       "      <td>2</td>\n",
       "      <td>18.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.761852</td>\n",
       "      <td>10.998228</td>\n",
       "      <td>3</td>\n",
       "      <td>226.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.762783</td>\n",
       "      <td>10.998978</td>\n",
       "      <td>4</td>\n",
       "      <td>345.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.763184</td>\n",
       "      <td>10.999331</td>\n",
       "      <td>5</td>\n",
       "      <td>397.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.763657</td>\n",
       "      <td>10.999795</td>\n",
       "      <td>6</td>\n",
       "      <td>461.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.764145</td>\n",
       "      <td>11.000370</td>\n",
       "      <td>7</td>\n",
       "      <td>531.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.764538</td>\n",
       "      <td>11.000874</td>\n",
       "      <td>8</td>\n",
       "      <td>590.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.764877</td>\n",
       "      <td>11.001401</td>\n",
       "      <td>9</td>\n",
       "      <td>646.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shp_2_1</td>\n",
       "      <td>45.765221</td>\n",
       "      <td>11.001960</td>\n",
       "      <td>10</td>\n",
       "      <td>704.023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence  shape_dist_traveled\n",
       "0  shp_2_1     45.760090     10.996771                  1                0.000\n",
       "1  shp_2_1     45.760235     10.996886                  2               18.672\n",
       "2  shp_2_1     45.761852     10.998228                  3              226.667\n",
       "3  shp_2_1     45.762783     10.998978                  4              345.598\n",
       "4  shp_2_1     45.763184     10.999331                  5              397.773\n",
       "5  shp_2_1     45.763657     10.999795                  6              461.719\n",
       "6  shp_2_1     45.764145     11.000370                  7              531.932\n",
       "7  shp_2_1     45.764538     11.000874                  8              590.560\n",
       "8  shp_2_1     45.764877     11.001401                  9              646.452\n",
       "9  shp_2_1     45.765221     11.001960                 10              704.023"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shapes_gtfs = pd.read_csv(\"gtfs-out/shapes.txt\")\n",
    "trips_gtfs = pd.read_csv(\"gtfs-out/trips.txt\")\n",
    "\n",
    "shapes_gtfs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_gtfs = trips_gtfs[[\"trip_id\",\"shape_id\"]]\n",
    "# merge columns on shape_id\n",
    "shapes_gtfs_merged = shapes_gtfs.merge(trips_gtfs, on=\"shape_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct trains is  13828 while the number of distinct shapes is  3639\n"
     ]
    }
   ],
   "source": [
    "# drop shape_id\n",
    "print(\"The number of distinct trains is \", shapes_gtfs_merged[\"trip_id\"].nunique(), \"while the number of distinct shapes is \", shapes_gtfs_merged[\"shape_id\"].nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could save some space by storing the shapes independently from the trains, since there's multiple trains that share the same shape. However, to keep the frontend of the website simple, we decided to store the shape of each train in a separated file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REG16144ALATRENTO', 'REG16148ALATRENTO', 'REG16150ALATRENTO', ...,\n",
       "       'FR9805TORINO P.NUOVALECCE',\n",
       "       'ICN36136REGGIO CALABRIA CENTRALETORINO P.NUOVA',\n",
       "       'ICN36140REGGIO CALABRIA CENTRALETORINO P.NUOVA'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(shapes_gtfs_merged[\"trip_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13828/13828 [00:38<00:00, 361.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# delete folder if exists\n",
    "import shutil\n",
    "if os.path.exists(\"dataset_generated/trains_shapes\"):\n",
    "    shutil.rmtree(\"dataset_generated/trains_shapes\")\n",
    "\n",
    "# create folder\n",
    "os.mkdir(\"dataset_generated/trains_shapes\")\n",
    "\n",
    "# for each trid_id, save all the corrisponding shapes in a file\n",
    "for trip_id, grouped_data in tqdm(shapes_gtfs_merged.groupby(\"trip_id\")):\n",
    "    grouped_data[[\"shape_pt_lat\", \"shape_pt_lon\"]].to_csv(\"dataset_generated/trains_shapes/{}.csv\".format(trip_id), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/giacomoorsi/DocsNotSynced/OpenRitardi/data/dataset_generated/trains_shapes.zip'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the folder with shutil\n",
    "shutil.make_archive(\"dataset_generated/trains_shapes\", 'zip', \"dataset_generated/trains_shapes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrenitaliaSpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
