{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling \n",
    "We explore the data to get a better understanding. The data is in `json` format. We want to covert the data to a table, in which every row is a stop of a train (e.g. the train RE 123 from Milano to Rome stopped at the station Firenze with a delay of 5 minutes). This should make queries easier and more efficient. The size of the data will grow but we will store in columnar format to enable compression. We rename all the columns in order to have a meaningful yet simple naming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "First, you need to download the data. In order to do so, go to [this](https://mega.nz/#F!vIAyDaTJ!PcLTFDbKaJaa0FZIEh5E-w) folder and download all (or only some) of the JSON files. Then, put them in the `DATA_FOLDER`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import DATA_FOLDER\n",
    "file = \"dati_01_01_2023.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import lit, col, to_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/22 22:11:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Trenitalia\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# set driver memory to 4GB\n",
    "spark.sparkContext._conf.setAll([('spark.driver.memory', '4g')])\n",
    "\n",
    "# get sc \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema is as follows:\n",
    "```\n",
    "{'type': 'struct',\n",
    " 'fields': [{'name': 'avvisiRFI',\n",
    "   'type': {'type': 'array',\n",
    "    'elementType': {'type': 'struct',\n",
    "     'fields': [{'name': 'corpo',\n",
    "       'type': 'string',\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'data', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'link', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'titolo', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "    'containsNull': True},\n",
    "   'nullable': True,\n",
    "   'metadata': {}},\n",
    "  {'name': 'avvisiTI',\n",
    "   'type': {'type': 'array',\n",
    "    'elementType': {'type': 'struct',\n",
    "     'fields': [{'name': 'corpo',\n",
    "       'type': 'string',\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'data', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'titolo', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "    'containsNull': True},\n",
    "   'nullable': True,\n",
    "   'metadata': {}},\n",
    "  {'name': 'giorno', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "  {'name': 'riassunto',\n",
    "   'type': {'type': 'struct',\n",
    "    'fields': [{'name': 'anticipoAccumulato',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'arrivoAnticipo',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'arrivoInOrario',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'arrivoInRitardo',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'circolazione',\n",
    "      'type': 'string',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'dataAggiornamento',\n",
    "      'type': 'string',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'numEC', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numEN', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numES', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numIC', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numNC', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'numREG', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "     {'name': 'partenzaInOrario',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'partenzaRitardo',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'ritardoAccumulato',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'ritardoArrivoTrenoPeggiore',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'ritardoPartenzaTrenoPeggiore',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniCancellati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniCircolati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniMonitorati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniRegolari',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'treniRiprogrammati',\n",
    "      'type': 'long',\n",
    "      'nullable': True,\n",
    "      'metadata': {}},\n",
    "     {'name': 'trenoPeggiore',\n",
    "      'type': 'string',\n",
    "      'nullable': True,\n",
    "      'metadata': {}}]},\n",
    "   'nullable': True,\n",
    "   'metadata': {}},\n",
    "  {'name': 'timeZone', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "  {'name': 'treni',\n",
    "   'type': {'type': 'array',\n",
    "    'elementType': {'type': 'struct',\n",
    "     'fields': [{'name': 'a',\n",
    "       'type': 'string',\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'c', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'cn', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'dl', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'fr',\n",
    "       'type': {'type': 'array',\n",
    "        'elementType': {'type': 'struct',\n",
    "         'fields': [{'name': 'n',\n",
    "           'type': 'string',\n",
    "           'nullable': True,\n",
    "           'metadata': {}},\n",
    "          {'name': 'oa', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "          {'name': 'op', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "          {'name': 'ra', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "          {'name': 'rp', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "        'containsNull': True},\n",
    "       'nullable': True,\n",
    "       'metadata': {}},\n",
    "      {'name': 'n', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'oa', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'oaz', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'od', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'oo', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'op', 'type': 'long', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'ope', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'opz', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'p', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'pr', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'ra', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'rp', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'sep', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
    "      {'name': 'sub', 'type': 'string', 'nullable': True, 'metadata': {}}]},\n",
    "    'containsNull': True},\n",
    "   'nullable': True,\n",
    "   'metadata': {}}]}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract the `treni` data\n",
    "There are multiple information in a json dump. We now consider only information about the trains, excluding riassunto e avvisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/01/2023\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(os.path.join(DATA_FOLDER, file))\n",
    "\n",
    "date = df.select(\"giorno\")\n",
    "date = date.select(\"giorno\").first().asDict()[\"giorno\"]\n",
    "print(date)\n",
    "\n",
    "day, month, year = date.split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the trains information\n",
    "trains = df.select(\"treni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first row in treni and extract dataframe using the specified schema in `schema`\n",
    "trains1 = trains.select(explode(\"treni\").alias(\"treni\"))\n",
    "\n",
    "# extract the struct of each row\n",
    "trains2 = trains1.select(\"treni.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----+----+--------------------+-----+----------+----+----------+----+----+----------+----+----------+-------------------+----+---+---+----+----+----+\n",
      "|             a|  c|  cn|  dl|                  fr|    n|        oa| oae|       oaz|  od|  oo|        op| ope|       opz|                  p|  pr| ra| rp| sea| sep| sub|\n",
      "+--------------+---+----+----+--------------------+-----+----------+----+----------+----+----+----------+----+----------+-------------------+----+---+---+----+----+----+\n",
      "|SESTRI LEVANTE|REG|null|null|[{LA SPEZIA CENTR...|12210|1672535400|null|1672535400|null|null|1672531200|null|1672531200| LA SPEZIA CENTRALE|null| -4|  3|null|null|null|\n",
      "|VENEZIA MESTRE|REG|null|null|[{VENEZIA SANTA L...|94314|1672532940|null|1672532940|null|null|1672532280|null|1672532280|VENEZIA SANTA LUCIA|null|  5|  0|null|null|null|\n",
      "|VENEZIA MESTRE|REG|null|null|[{VENEZIA SANTA L...|94308|1672534980|null|1672534980|null|null|1672534320|null|1672534320|VENEZIA SANTA LUCIA|null|  1|  0|null|null|null|\n",
      "|VENEZIA MESTRE|REG|null|null|[{VENEZIA SANTA L...|93338|1672536060|null|1672536060|null|null|1672535400|null|1672535400|VENEZIA SANTA LUCIA|null|  2|  1|null|null|null|\n",
      "|VENEZIA MESTRE|REG|null|null|[{VENEZIA SANTA L...|93318|1672537320|null|1672537320|null|null|1672536660|null|1672536660|VENEZIA SANTA LUCIA|null|  1|  0|null|null|null|\n",
      "+--------------+---+----+----+--------------------+-----+----------+----+----------+----+----+----------+----+----------+-------------------+----+---+---+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trains2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'c', 'cn', 'dl', 'fr', 'n', 'oa', 'oae', 'oaz', 'od', 'oo', 'op', 'ope', 'opz', 'p', 'pr', 'ra', 'rp', 'sea', 'sep', 'sub']\n"
     ]
    }
   ],
   "source": [
    "# show list of columns\n",
    "print(list(trains2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapper = {\n",
    "    \"a\": \"train_arrival_stop_name\", \n",
    "    \"c\": \"train_class\",\n",
    "    \"cn\" : \"train_cn\", # unknown\n",
    "    \"dl\": \"train_dl\", # unknown\n",
    "    \"fr\": \"stops\",\n",
    "    \"n\" : \"train_number\",\n",
    "    \"oa\": \"train_arrival_time\",\n",
    "    \"oaz\": \"train_oaz\", # unknown\n",
    "    \"od\": \"train_od\", # unknown\n",
    "    \"oo\" : \"train_oo\", # unknown\n",
    "    \"op\": \"train_departure_time\",\n",
    "    \"ope\" : \"train_ope\", # unknown\n",
    "    \"opz\" : \"train_opz\", # unknown\n",
    "    \"p\" : \"train_departure_stop_name\",\n",
    "    \"pr\": \"train_pr\", # unknown\n",
    "    \"ra\": \"train_arrival_delay\",\n",
    "    \"rp\" : \"train_departure_delay\",\n",
    "    \"sep\" : \"train_sep\", # unknown\n",
    "    \"sub\" : \"train_sub\" # unknown\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------+--------+--------+--------------------+------------+------------------+----+----------+--------+--------+--------------------+---------+----------+-------------------------+--------+-------------------+---------------------+----+---------+---------+\n",
      "|train_arrival_stop_name|train_class|train_cn|train_dl|               stops|train_number|train_arrival_time| oae| train_oaz|train_od|train_oo|train_departure_time|train_ope| train_opz|train_departure_stop_name|train_pr|train_arrival_delay|train_departure_delay| sea|train_sep|train_sub|\n",
      "+-----------------------+-----------+--------+--------+--------------------+------------+------------------+----+----------+--------+--------+--------------------+---------+----------+-------------------------+--------+-------------------+---------------------+----+---------+---------+\n",
      "|         SESTRI LEVANTE|        REG|    null|    null|[{LA SPEZIA CENTR...|       12210|        1672535400|null|1672535400|    null|    null|          1672531200|     null|1672531200|       LA SPEZIA CENTRALE|    null|                 -4|                    3|null|     null|     null|\n",
      "|         VENEZIA MESTRE|        REG|    null|    null|[{VENEZIA SANTA L...|       94314|        1672532940|null|1672532940|    null|    null|          1672532280|     null|1672532280|      VENEZIA SANTA LUCIA|    null|                  5|                    0|null|     null|     null|\n",
      "|         VENEZIA MESTRE|        REG|    null|    null|[{VENEZIA SANTA L...|       94308|        1672534980|null|1672534980|    null|    null|          1672534320|     null|1672534320|      VENEZIA SANTA LUCIA|    null|                  1|                    0|null|     null|     null|\n",
      "|         VENEZIA MESTRE|        REG|    null|    null|[{VENEZIA SANTA L...|       93338|        1672536060|null|1672536060|    null|    null|          1672535400|     null|1672535400|      VENEZIA SANTA LUCIA|    null|                  2|                    1|null|     null|     null|\n",
      "|         VENEZIA MESTRE|        REG|    null|    null|[{VENEZIA SANTA L...|       93318|        1672537320|null|1672537320|    null|    null|          1672536660|     null|1672536660|      VENEZIA SANTA LUCIA|    null|                  1|                    0|null|     null|     null|\n",
      "+-----------------------+-----------+--------+--------+--------------------+------------+------------------+----+----------+--------+--------+--------------------+---------+----------+-------------------------+--------+-------------------+---------------------+----+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename all columns\n",
    "trains3 = trains2\n",
    "for k, v in columns_mapper.items():\n",
    "    trains3 = trains3.withColumnRenamed(k, v)\n",
    "\n",
    "trains3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_columns_mapper = {\n",
    "    \"n\": \"stop_name\",\n",
    "    \"oa\": \"stop_arrival_time\",\n",
    "    \"op\": \"stop_departure_time\",\n",
    "    \"ra\": \"stop_arrival_delay\",\n",
    "    \"rp\": \"stop_departure_delay\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------+--------+--------+------------+------------------+----+----------+--------+--------+--------------------+---------+----------+-------------------------+--------+-------------------+---------------------+----+---------+---------+------------------+-----------------+-------------------+------------------+--------------------+\n",
      "|train_arrival_stop_name|train_class|train_cn|train_dl|train_number|train_arrival_time| oae| train_oaz|train_od|train_oo|train_departure_time|train_ope| train_opz|train_departure_stop_name|train_pr|train_arrival_delay|train_departure_delay| sea|train_sep|train_sub|         stop_name|stop_arrival_time|stop_departure_time|stop_arrival_delay|stop_departure_delay|\n",
      "+-----------------------+-----------+--------+--------+------------+------------------+----+----------+--------+--------+--------------------+---------+----------+-------------------------+--------+-------------------+---------------------+----+---------+---------+------------------+-----------------+-------------------+------------------+--------------------+\n",
      "|         SESTRI LEVANTE|        REG|    null|    null|       12210|        1672535400|null|1672535400|    null|    null|          1672531200|     null|1672531200|       LA SPEZIA CENTRALE|    null|                 -4|                    3|null|     null|     null|LA SPEZIA CENTRALE|                0|         1672531200|                 N|                   3|\n",
      "|         SESTRI LEVANTE|        REG|    null|    null|       12210|        1672535400|null|1672535400|    null|    null|          1672531200|     null|1672531200|       LA SPEZIA CENTRALE|    null|                 -4|                    3|null|     null|     null|       RIOMAGGIORE|       1672531620|         1672531680|              n.d.|                   3|\n",
      "|         SESTRI LEVANTE|        REG|    null|    null|       12210|        1672535400|null|1672535400|    null|    null|          1672531200|     null|1672531200|       LA SPEZIA CENTRALE|    null|                 -4|                    3|null|     null|     null|          MANAROLA|       1672531860|         1672531920|              n.d.|                   2|\n",
      "|         SESTRI LEVANTE|        REG|    null|    null|       12210|        1672535400|null|1672535400|    null|    null|          1672531200|     null|1672531200|       LA SPEZIA CENTRALE|    null|                 -4|                    3|null|     null|     null|         CORNIGLIA|       1672532100|         1672532160|                -1|                   1|\n",
      "|         SESTRI LEVANTE|        REG|    null|    null|       12210|        1672535400|null|1672535400|    null|    null|          1672531200|     null|1672531200|       LA SPEZIA CENTRALE|    null|                 -4|                    3|null|     null|     null|          VERNAZZA|       1672532400|         1672532460|              n.d.|                   2|\n",
      "+-----------------------+-----------+--------+--------+------------+------------------+----+----------+--------+--------+--------------------+---------+----------+-------------------------+--------+-------------------+---------------------+----+---------+---------+------------------+-----------------+-------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the stops are stored in an array of structs ([{...}, {...}, ...]). \n",
    "# by using explode, we can extract the struct and create a new row for each stop\n",
    "# we then drop the original array of structs `stops` and we extract from the structs {...} the columns by using \"stops_extracted.*\"\n",
    "# we then drop the original struct and we rename the columns using the mapper \"stops_columns_mapper\n",
    "trains4 = trains3.select(\"*\", explode(\"stops\").alias(\"stops_extracted\")) \\\n",
    "    .drop(\"stops\") \\\n",
    "    .select(\"*\", \"stops_extracted.*\") \\\n",
    "    .drop(\"stops_extracted\")\n",
    "\n",
    "for k, v in stops_columns_mapper.items():\n",
    "    trains4 = trains4.withColumnRenamed(k, v)\n",
    "\n",
    "trains4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----------+\n",
      "|day|month|year|      date|\n",
      "+---+-----+----+----------+\n",
      "| 01|   01|2023|2023-01-01|\n",
      "+---+-----+----+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trains5 = trains4.withColumn(\"day\", lit(day)).withColumn(\"month\", lit(month)).withColumn(\"year\", lit(year)).withColumn(\"date\", lit(date))\n",
    "\n",
    "# convert day to date format \n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "trains6 = trains5.withColumn(\"date\", to_date(\"date\", \"dd/MM/yyyy\"))\n",
    "trains6.select(col(\"day\"), col(\"month\"), col(\"year\"), col(\"date\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all done! now this should be done for all the files in the folder and then we can save the data in parquet format\n",
    "# we can then use the parquet files to do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_day = day.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/22 22:24:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# store the new dataframe in parquet format\n",
    "trains6.write.parquet(os.path.join(DATA_FOLDER, \"parquet\", new_day + \".parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet file\n",
    "trains7 = spark.read.parquet(os.path.join(DATA_FOLDER, \"parquet\", new_day + \".parquet\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a big dataframe containing all the daily datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the code of point 1, we create a big dataframe combining all the daily datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: read all the datasets, extract the `treni` data, add a column with the date for each one of them, and then apply the other transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_daily_dataset(file) -> pyspark.sql.DataFrame: \n",
    "    \"\"\"Here we just add the date column and we explore \"treni\" column\"\"\"\n",
    "    df = spark.read.json(os.path.join(DATA_FOLDER, file), multiLine=True)\n",
    "    date = df.select(\"giorno\")\n",
    "    date = date.select(\"giorno\").first().asDict()[\"giorno\"]\n",
    "    day, month, year = date.split(\"/\")\n",
    "\n",
    "    df = df.select(\"treni\").select(explode(\"treni\").alias(\"treni\")).select(\"treni.*\")\n",
    "    df = df.withColumn(\"day\", lit(day)).withColumn(\"month\", lit(month)).withColumn(\"year\", lit(year)).withColumn(\"date\", lit(date))\n",
    "    df = df.withColumn(\"date\", to_date(\"date\", \"dd/MM/yyyy\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_daily_datasets() -> pyspark.sql.DataFrame: \n",
    "    \"\"\"Reads all daily datasets and union them after applying daily preprocessing\"\"\"\n",
    "    all_files = os.listdir(DATA_FOLDER)\n",
    "    all_files = [f for f in all_files if f.endswith(\".json\")]\n",
    "    all_files.sort()\n",
    "    df = read_preprocess_daily_dataset(all_files[0])\n",
    "    for f in tqdm(all_files[1:]):\n",
    "        df = df.unionByName(read_preprocess_daily_dataset(f), allowMissingColumns=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_file_name(file_name: str) -> str:\n",
    "    \"\"\"Extracts the date from the file name\"\"\"\n",
    "    date = file_name.split(\"_\")[1:]\n",
    "    date = \"_\".join(date)\n",
    "    date = date.split(\".\")[0]\n",
    "    # convert to yyyy-mm-dd format\n",
    "    day, month, year = date.split(\"_\")\n",
    "    date = \"{}-{}-{}\".format(year, month, day)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_available_files() -> List[str]:\n",
    "    \"\"\"Returns the list of files that are available in the data folder\"\"\"\n",
    "    all_files = os.listdir(DATA_FOLDER)\n",
    "    all_files = [f for f in all_files if f.endswith(\".json\")]\n",
    "    all_files.sort(key= lambda x: get_date_from_file_name(x))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 365 files in the data folder\n",
      "The first file is dati_2023_02_01.json\n",
      "The last file is dati_2023_12_31.json\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} files in the data folder\".format(len(get_available_files())))\n",
    "print(\"The first file is {}\".format(get_available_files()[0]))\n",
    "print(\"The last file is {}\".format(get_available_files()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364/364 [06:39<00:00,  1.10s/it]                               \n"
     ]
    }
   ],
   "source": [
    "df = read_all_daily_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame: \n",
    "    \"\"\"Procesess the entire dataset of multiple days\"\"\"\n",
    "\n",
    "    columns_mapper = {\n",
    "        \"a\": \"train_arrival_stop_name\", \n",
    "        \"c\": \"train_class\",\n",
    "        \"cn\" : \"train_cn\", # unknown\n",
    "        \"dl\": \"train_dl\", # unknown\n",
    "        \"fr\": \"stops\",\n",
    "        \"n\" : \"train_number\",\n",
    "        \"oa\": \"train_arrival_time\",\n",
    "        \"oaz\": \"train_oaz\", # unknown\n",
    "        \"od\": \"train_od\", # unknown\n",
    "        \"oo\" : \"train_oo\", # unknown\n",
    "        \"op\": \"train_departure_time\",\n",
    "        \"ope\" : \"train_ope\", # unknown\n",
    "        \"opz\" : \"train_opz\", # unknown\n",
    "        \"p\" : \"train_departure_stop_name\",\n",
    "        \"pr\": \"train_pr\", # unknown\n",
    "        \"ra\": \"train_arrival_delay\",\n",
    "        \"rp\" : \"train_departure_delay\",\n",
    "        \"sep\" : \"train_sep\", # unknown\n",
    "        \"sub\" : \"train_sub\" # unknown\n",
    "    }\n",
    "\n",
    "    # rename all columns\n",
    "    for k, v in columns_mapper.items():\n",
    "        df = df.withColumnRenamed(k, v)\n",
    "\n",
    "    stops_columns_mapper = {\n",
    "        \"n\": \"stop_name\",\n",
    "        \"oa\": \"stop_arrival_time\",\n",
    "        \"op\": \"stop_departure_time\",\n",
    "        \"ra\": \"stop_arrival_delay\",\n",
    "        \"rp\": \"stop_departure_delay\"\n",
    "    }\n",
    "\n",
    "    df = df.select(\"*\", explode(\"stops\").alias(\"stops_extracted\")) \\\n",
    "    .drop(\"stops\") \\\n",
    "    .select(\"*\", \"stops_extracted.*\") \\\n",
    "    .drop(\"stops_extracted\")\n",
    "\n",
    "    for k, v in stops_columns_mapper.items():\n",
    "        df = df.withColumnRenamed(k, v)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = preprocess_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the data\n",
    "Here we store all data in a joined Parquet dataset, which can be used for downstream analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/22 22:34:55 WARN DAGScheduler: Broadcasting large task binary with size 17.6 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# store in columnar format\n",
    "df2.write.parquet(os.path.join(DATA_FOLDER, \"parquet\", \"all.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrenitaliaSpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbad97b5d24366a386ca7fadb2084e585e66698cec63432bb0202e3b6a0edfdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
